{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d161fc-121d-4fe3-91e0-591680572adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import lax as jlax\n",
    "from jax.tree_util import register_pytree_node_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cf75e38-73d4-4608-b614-50ed37afd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class Parameter:\n",
    "    \"\"\"\n",
    "    A class to represent a parameter with a name and a value, supporting basic arithmetic operations\n",
    "    and integration with the JAX pytree system.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the parameter.\n",
    "    value : ndarray\n",
    "        The value of the parameter, typically a NumPy array or similar.\n",
    "    shape : tuple\n",
    "        The shape of the value.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,value):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        self.shape = value.shape\n",
    "    def __sub__(self,param):\n",
    "        if isinstance(param,Parameter):\n",
    "            return Parameter(self.name,self.value-param.value)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(param)} and 'Parameter'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,Parameter):\n",
    "            return Parameter(self.name,self.value+other.value)\n",
    "        if isinstance(other,float):\n",
    "            return Parameter(self.name,self.value+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'Parameter'\")\n",
    "    def __mul__(self,factor):\n",
    "        if isinstance(factor,float):\n",
    "            return Parameter(self.name,factor*self.value)\n",
    "        raise TypeError(f'Cannot multiply a Parameter with {type(factor)}')\n",
    "    def __rmul__(self,factor):\n",
    "        if isinstance(factor,float):\n",
    "            return Parameter(self.name,factor*self.value)\n",
    "        raise TypeError(f'Cannot multiply a Parameter with {type(factor)}')\n",
    "    def __pow__(self,factor):\n",
    "        return Parameter(self.name,self.value**factor)\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,Parameter):\n",
    "            return Parameter(self.name,self.value/other.value)\n",
    "        if isinstance(other,float):\n",
    "            return Parameter(self.name,self.value/other)\n",
    "        raise TypeError(f'Cannot divide a Parameter with {type(other)}')\n",
    "    def tree_flatten(self):\n",
    "        children = (self.value,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07cc88c-ebbf-43f6-8def-3f00b1ce1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class LinearParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a linear layer, specifically the weights, which are stored\n",
    "    as a Parameter object.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the linear layer parameters.\n",
    "    weights : Parameter\n",
    "        The weights of the linear layer, stored as a Parameter object.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,weights):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"W\",weights)\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,LinearParams):\n",
    "            return LinearParams(self.name,self.weights-other.weights)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'LinearParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,LinearParams) :\n",
    "            return LinearParams(self.name,self.weights+other.weights)\n",
    "        if isinstance(other,float) :\n",
    "            return LinearParams(self.name,self.weights+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'LinearParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'LinearParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'LinearParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,LinearParams) :\n",
    "            return LinearParams(self.name,self.weights/other.weights)\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights/other)\n",
    "        raise TypeError(f\"Cannot divide a 'LinearParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return LinearParams(self.name,self.weights**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9bf3d6-6227-4498-a5fa-3126765d03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class FeedForwardParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a feedforward neural network layer, specifically the weights and biases,\n",
    "    which are stored as Parameter objects.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the feedforward layer parameters.\n",
    "    weights : Parameter\n",
    "        The weights of the feedforward layer, stored as a Parameter object.\n",
    "    bias : Parameter\n",
    "        The bias of the feedforward layer, stored as a Parameter object.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,weights,bias):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"W\",weights)\n",
    "        if isinstance(bias,Parameter):\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = Parameter(\"bais\",bias)\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,FeedForwardParams) :\n",
    "            return FeedForwardParams(self.name,self.weights-other.weights,self.bias-other.bias)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'FeedForwardParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,FeedForwardParams):\n",
    "            return FeedForwardParams(self.name,self.weights+other.weights,self.bias+other.bias)\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights+other,self.bias+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'FeedForwardParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights*other,self.bias*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights*other,self.bias*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,FeedForwardParams):\n",
    "            return FeedForwardParams(self.name,self.weights/other.weights,self.bias/other.bias)\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights/other,self.bias/other)\n",
    "        raise TypeError(f\"Cannot divide a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return FeedForwardParams(self.name,self.weights**factor,self.bias**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,self.bias,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a8bc36-a5ec-483e-b4d4-2de7dbd92315",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class AttentionParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of an attention mechanism, specifically the weights for key, query, and value,\n",
    "    which are stored as Parameter objects.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the attention parameters.\n",
    "    w_k : Parameter\n",
    "        The weights for the key, stored as a Parameter object.\n",
    "    w_q : Parameter\n",
    "        The weights for the query, stored as a Parameter object.\n",
    "    w_v : Parameter\n",
    "        The weights for the value, stored as a Parameter object.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,w_k,w_q,w_v):\n",
    "        self.name = name\n",
    "        if isinstance(w_k,Parameter):\n",
    "            self.w_k = w_k\n",
    "        else:\n",
    "            self.w_k = Parameter(\"w_k\",w_k)\n",
    "        if isinstance(w_q,Parameter):\n",
    "            self.w_q = w_q\n",
    "        else:\n",
    "            self.w_q = Parameter(\"w_q\",w_q)\n",
    "        if isinstance(w_v,Parameter):\n",
    "            self.w_v = w_v\n",
    "        else:\n",
    "            self.w_v = Parameter(\"w_v\",w_v)\n",
    "        \n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k-other.w_k,self.w_q-other.w_q,self.w_v-other.w_v)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'AttentionParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k+other.w_k,self.w_q+other.w_q,self.w_v+other.w_v)\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k+other,self.w_q+other,self.w_v+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'AttentionParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k*other,self.w_q*other,self.w_v*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'AttentionParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k/other.w_k,self.w_q/other.w_q,self.w_v/other.w_v)\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k/other,self.w_q/other,self.w_v/other)\n",
    "        raise TypeError(f\"Cannot divide a 'AttentionParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k*other,self.w_q*other,self.w_v*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'AttentionParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return AttentionParams(self.name,self.w_k**factor,self.w_q**factor,self.w_v**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.w_k,self.w_q,self.w_v,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648df40c-3dfc-4421-947b-47994ef7d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class MultiHeadAttentionParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a multi-head attention mechanism, including the output weights\n",
    "    and a list of individual attention heads.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the multi-head attention parameters.\n",
    "    weights : Parameter\n",
    "        The output weights of the multi-head attention, stored as a Parameter object.\n",
    "    heads : list[AttentionParams]\n",
    "        A list of AttentionParams objects representing the individual attention heads.\n",
    "    num_heads : int\n",
    "        The number of attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,weights,heads:list[AttentionParams]):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"Wo\",weights)\n",
    "        self.heads = heads\n",
    "        self.num_heads = len(heads)\n",
    "    def add(self,head1,head2):\n",
    "        return head1+head2\n",
    "    def subtract(self,head1,head2):\n",
    "        return head1-head2\n",
    "    def multiply(self,val,head):\n",
    "        return val*head\n",
    "    def divide(self,head,val):\n",
    "        return head/val\n",
    "    def pow(self,val,head):\n",
    "        return head**val\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams):\n",
    "            heads = list(map(self.subtract,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights-other.weights,heads)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'MultiHeadAttentionParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams) :\n",
    "            heads = list(map(self.add,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights+other.weights,heads)\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.add,self.heads,[other]*self.num_heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights+other,heads)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'MultiHeadAttentionParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.multiply,[other]*self.num_heads,self.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights*other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams) :\n",
    "            heads = list(map(self.divide,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights/other.weights,heads)\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.divide,self.heads,[other]*self.num_heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights/other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.multiply,[other]*self.num_heads,self.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights*other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        heads = list(map(self.pow,[factor]*self.num_heads,self.heads))\n",
    "        return MultiHeadAttentionParams(self.name,self.weights**factor,heads)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,self.heads,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ce0f30-f80f-41fb-85c1-df8b4319ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class ModuleParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a module, including a list of its components.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the module parameters.\n",
    "    components : list\n",
    "        A list of components that make up the module.\n",
    "    num_comps : int\n",
    "        The number of components in the module.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,components):\n",
    "        self.name = name\n",
    "        self.components = components\n",
    "        self.num_comps = len(components)\n",
    "    def multiply(self,val,comp):\n",
    "        return val*comp\n",
    "    def subtract(self,comp1,comp2):\n",
    "        return comp1-comp2\n",
    "    def add(self,comp1,comp2):\n",
    "        return comp1+comp2\n",
    "    def pow(self,val,comp):\n",
    "        return comp**val\n",
    "    def divide(self,comp,val):\n",
    "        return comp/val\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,ModuleParams):\n",
    "            comps = list(map(self.subtract,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'ModuleParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,ModuleParams) :\n",
    "            comps = list(map(self.add,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.add,self.components,[other]*self.num_comps))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'ModuleParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.multiply,[other]*self.num_comps,self.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot multiply a 'ModuleParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,ModuleParams) :\n",
    "            comps = list(map(self.divide,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.divide,self.components,[other]*self.num_comps))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot divide a 'ModuleParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.multiply,[other]*self.num_comps,self.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot multiply a 'ModuleParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        comps = list(map(self.pow,[factor]*self.num_comps,self.components))\n",
    "        return ModuleParams(self.name,comps)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.components,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6f23684-0ff4-4bc5-8d71-3833852d2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = Parameter(\"W\",random.normal(random.key(29),shape=(24,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2cbf554-de59-4193-9700-95c840e4eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = Parameter(\"dw\",random.normal(random.key(29),shape=(24,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69414672-ff96-4bba-8131-daf4b329e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights3 = (weights1 - 0.1*weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4bff82a-fa8a-4f29-a755-19ab18b64216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(weights2,Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92213b13-0b26-4fd2-8085-ed25cb6d2588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.8999999 , 0.90000004, 0.90000004, 0.9       , 0.9       ,\n",
       "        0.9       , 0.90000004, 0.90000004, 0.9       , 0.90000004,\n",
       "        0.9       , 0.90000004, 0.9       , 0.9       , 0.9       ,\n",
       "        0.9       ],\n",
       "       [0.90000004, 0.9       , 0.90000004, 0.9       , 0.9       ,\n",
       "        0.9       , 0.90000004, 0.9       , 0.9       , 0.9       ,\n",
       "        0.9       , 0.9       , 0.90000004, 0.90000004, 0.90000004,\n",
       "        0.90000004],\n",
       "       [0.90000004, 0.90000004, 0.90000004, 0.90000004, 0.8999999 ,\n",
       "        0.90000004, 0.9000001 , 0.90000004, 0.9000001 , 0.9       ,\n",
       "        0.90000004, 0.9       , 0.9       , 0.9       , 0.9000001 ,\n",
       "        0.8999999 ],\n",
       "       [0.90000004, 0.90000004, 0.9000001 , 0.9000001 , 0.90000004,\n",
       "        0.9       , 0.9       , 0.90000004, 0.90000004, 0.9       ,\n",
       "        0.90000004, 0.90000004, 0.9       , 0.9       , 0.90000004,\n",
       "        0.9       ],\n",
       "       [0.9       , 0.90000004, 0.9       , 0.9       , 0.90000004,\n",
       "        0.90000004, 0.90000004, 0.8999999 , 0.90000004, 0.90000004,\n",
       "        0.90000004, 0.9       , 0.9       , 0.90000004, 0.90000004,\n",
       "        0.9       ],\n",
       "       [0.90000004, 0.90000004, 0.8999999 , 0.9       , 0.90000004,\n",
       "        0.8999999 , 0.8999999 , 0.90000004, 0.9       , 0.8999999 ,\n",
       "        0.90000004, 0.90000004, 0.9       , 0.9000001 , 0.9       ,\n",
       "        0.90000004],\n",
       "       [0.8999999 , 0.90000004, 0.9       , 0.9       , 0.9       ,\n",
       "        0.9       , 0.9       , 0.9       , 0.90000004, 0.90000004,\n",
       "        0.9       , 0.90000004, 0.9       , 0.90000004, 0.9       ,\n",
       "        0.9       ],\n",
       "       [0.90000004, 0.9       , 0.9       , 0.9       , 0.9       ,\n",
       "        0.90000004, 0.9       , 0.90000004, 0.9       , 0.9       ,\n",
       "        0.9       , 0.9       , 0.90000004, 0.8999999 , 0.9       ,\n",
       "        0.9       ],\n",
       "       [0.90000004, 0.8999999 , 0.9       , 0.9       , 0.9       ,\n",
       "        0.9       , 0.90000004, 0.9       , 0.90000004, 0.90000004,\n",
       "        0.9       , 0.9       , 0.9       , 0.90000004, 0.90000004,\n",
       "        0.90000004],\n",
       "       [0.9       , 0.9       , 0.9       , 0.9       , 0.9       ,\n",
       "        0.90000004, 0.90000004, 0.9       , 0.9       , 0.90000004,\n",
       "        0.9       , 0.90000004, 0.90000004, 0.9       , 0.90000004,\n",
       "        0.9       ],\n",
       "       [0.9       , 0.9       , 0.90000004, 0.9       , 0.90000004,\n",
       "        0.90000004, 0.90000004, 0.9       , 0.90000004, 0.9       ,\n",
       "        0.9       , 0.9       , 0.9       , 0.8999999 , 0.9       ,\n",
       "        0.90000004],\n",
       "       [0.90000004, 0.90000004, 0.9       , 0.90000004, 0.9       ,\n",
       "        0.90000004, 0.9       , 0.90000004, 0.90000004, 0.9       ,\n",
       "        0.9000001 , 0.9       , 0.9       , 0.9       , 0.90000004,\n",
       "        0.9000001 ],\n",
       "       [0.9       , 0.90000004, 0.9       , 0.8999999 , 0.90000004,\n",
       "        0.90000004, 0.90000004, 0.9       , 0.90000004, 0.90000004,\n",
       "        0.9       , 0.9000001 , 0.9       , 0.90000004, 0.9       ,\n",
       "        0.9       ],\n",
       "       [0.9       , 0.9       , 0.90000004, 0.90000004, 0.9       ,\n",
       "        0.9000001 , 0.90000004, 0.90000004, 0.9       , 0.9       ,\n",
       "        0.9       , 0.9       , 0.8999999 , 0.90000004, 0.90000004,\n",
       "        0.9       ],\n",
       "       [0.90000004, 0.90000004, 0.9       , 0.90000004, 0.9       ,\n",
       "        0.9       , 0.9       , 0.90000004, 0.9       , 0.9       ,\n",
       "        0.90000004, 0.9       , 0.90000004, 0.9       , 0.9       ,\n",
       "        0.9       ],\n",
       "       [0.90000004, 0.90000004, 0.90000004, 0.9       , 0.8999999 ,\n",
       "        0.9       , 0.9       , 0.9       , 0.9       , 0.9       ,\n",
       "        0.90000004, 0.90000004, 0.9       , 0.9       , 0.9       ,\n",
       "        0.90000004],\n",
       "       [0.9       , 0.90000004, 0.90000004, 0.90000004, 0.9       ,\n",
       "        0.9       , 0.9       , 0.9       , 0.9       , 0.90000004,\n",
       "        0.9       , 0.9       , 0.9       , 0.8999999 , 0.90000004,\n",
       "        0.9       ],\n",
       "       [0.8999999 , 0.9       , 0.90000004, 0.90000004, 0.90000004,\n",
       "        0.90000004, 0.90000004, 0.90000004, 0.9       , 0.9000001 ,\n",
       "        0.9       , 0.90000004, 0.90000004, 0.90000004, 0.90000004,\n",
       "        0.90000004],\n",
       "       [0.9       , 0.9       , 0.9       , 0.90000004, 0.9       ,\n",
       "        0.9       , 0.90000004, 0.9       , 0.9       , 0.9000001 ,\n",
       "        0.8999999 , 0.8999999 , 0.9       , 0.90000004, 0.8999999 ,\n",
       "        0.9       ],\n",
       "       [0.90000004, 0.9000001 , 0.8999999 , 0.90000004, 0.9       ,\n",
       "        0.90000004, 0.9       , 0.90000004, 0.90000004, 0.90000004,\n",
       "        0.9       , 0.9       , 0.9       , 0.90000004, 0.9       ,\n",
       "        0.90000004],\n",
       "       [0.8999999 , 0.9       , 0.9       , 0.9       , 0.9       ,\n",
       "        0.9       , 0.90000004, 0.9       , 0.9       , 0.90000004,\n",
       "        0.9       , 0.90000004, 0.90000004, 0.9       , 0.9       ,\n",
       "        0.90000004],\n",
       "       [0.9       , 0.90000004, 0.90000004, 0.90000004, 0.9       ,\n",
       "        0.9       , 0.90000004, 0.9       , 0.9       , 0.9       ,\n",
       "        0.90000004, 0.9000001 , 0.90000004, 0.90000004, 0.90000004,\n",
       "        0.9       ],\n",
       "       [0.9       , 0.90000004, 0.90000004, 0.9       , 0.9       ,\n",
       "        0.90000004, 0.9000001 , 0.90000004, 0.90000004, 0.9       ,\n",
       "        0.9       , 0.90000004, 0.9       , 0.9       , 0.90000004,\n",
       "        0.9000001 ],\n",
       "       [0.90000004, 0.9       , 0.9       , 0.90000004, 0.9       ,\n",
       "        0.90000004, 0.9       , 0.9       , 0.90000004, 0.9       ,\n",
       "        0.9       , 0.9000001 , 0.90000004, 0.8999999 , 0.9       ,\n",
       "        0.9       ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights3/weights1).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bf371-910c-4850-a86e-5bd1b5d93329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
