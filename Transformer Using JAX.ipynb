{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3a98fd-e195-4e24-a1ea-4f537a90622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "import jax\n",
    "from jax import grad,vmap\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.tree_util import register_pytree_node_class\n",
    "import numpy as np\n",
    "from jax import lax as jlax\n",
    "from jax.tree_util import register_pytree_node_class\n",
    "import json\n",
    "#import copyself.components\n",
    "import jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "771ee6e4-ef9c-4f43-a667-45d8a26d1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class Parameter:\n",
    "    def __init__(self,name,value):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        self.shape = value.shape\n",
    "    def __sub__(self,param):\n",
    "        if isinstance(param,Parameter):\n",
    "            return Parameter(self.name,self.value-param.value)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(param)} and 'Parameter'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,Parameter):\n",
    "            return Parameter(self.name,self.value+other.value)\n",
    "        if isinstance(other,float):\n",
    "            return Parameter(self.name,self.value+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'Parameter'\")\n",
    "    def __mul__(self,factor):\n",
    "        if isinstance(factor,float):\n",
    "            return Parameter(self.name,factor*self.value)\n",
    "        raise TypeError(f'Cannot multiply a Parameter with {type(factor)}')\n",
    "    def __rmul__(self,factor):\n",
    "        if isinstance(factor,float):\n",
    "            return Parameter(self.name,factor*self.value)\n",
    "        raise TypeError(f'Cannot multiply a Parameter with {type(factor)}')\n",
    "    def __pow__(self,factor):\n",
    "        return Parameter(self.name,self.value**factor)\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,Parameter):\n",
    "            return Parameter(self.name,self.value/other.value)\n",
    "        if isinstance(other,float):\n",
    "            return Parameter(self.name,self.value/other)\n",
    "        raise TypeError(f'Cannot divide a Parameter with {type(factor)}')\n",
    "    def tree_flatten(self):\n",
    "        children = (self.value,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0ae3dc55-f13a-4fa3-add9-e59aa6565d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class LinearParams:\n",
    "    def __init__(self,name,weights):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"W\",weights)\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,LinearParams):\n",
    "            return LinearParams(self.name,self.weights-other.weights)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'LinearParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,LinearParams) :\n",
    "            return LinearParams(self.name,self.weights+other.weights)\n",
    "        if isinstance(other,float) :\n",
    "            return LinearParams(self.name,self.weights+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'LinearParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'LinearParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'LinearParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,LinearParams) :\n",
    "            return LinearParams(self.name,self.weights/other.weights)\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights/other)\n",
    "        raise TypeError(f\"Cannot divide a 'LinearParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return LinearParams(self.name,self.weights**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "6a02a9f5-9ff6-47ba-972f-dbb93a51396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class FeedForwardParams:\n",
    "    def __init__(self,name,weights,bias):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"W\",weights)\n",
    "        if isinstance(bias,Parameter):\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = Parameter(\"bais\",bias)\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,FeedForwardParams) :\n",
    "            return FeedForwardParams(self.name,self.weights-other.weights,self.bias-other.bias)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'FeedForwardParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,FeedForwardParams):\n",
    "            return FeedForwardParams(self.name,self.weights+other.weights,self.bias+other.bias)\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights+other,self.bias+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'FeedForwardParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights*other,self.bias*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights*other,self.bias*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,FeedForwardParams):\n",
    "            return FeedForwardParams(self.name,self.weights/other.weights,self.bias/other.bias)\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights/other,self.bias/other)\n",
    "        raise TypeError(f\"Cannot divide a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return FeedForwardParams(self.name,self.weights**factor,self.bias**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,self.bias,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "79eceadc-72b0-4905-8719-c53652142344",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class AttentionParams:\n",
    "    def __init__(self,name,w_k,w_q,w_v):\n",
    "        self.name = name\n",
    "        if isinstance(w_k,Parameter):\n",
    "            self.w_k = w_k\n",
    "        else:\n",
    "            self.w_k = Parameter(\"w_k\",w_k)\n",
    "        if isinstance(w_q,Parameter):\n",
    "            self.w_q = w_q\n",
    "        else:\n",
    "            self.w_q = Parameter(\"w_q\",w_q)\n",
    "        if isinstance(w_v,Parameter):\n",
    "            self.w_v = w_v\n",
    "        else:\n",
    "            self.w_v = Parameter(\"w_v\",w_v)\n",
    "        \n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k-other.w_k,self.w_q-other.w_q,self.w_v-other.w_v)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'AttentionParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k+other.w_k,self.w_q+other.w_q,self.w_v+other.w_v)\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k+other,self.w_q+other,self.w_v+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'AttentionParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k*other,self.w_q*other,self.w_v*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'AttentionParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k/other.w_k,self.w_q/other.w_q,self.w_v/other.w_v)\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k/other,self.w_q/other,self.w_v/other)\n",
    "        raise TypeError(f\"Cannot divide a 'AttentionParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k*other,self.w_q*other,self.w_v*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'AttentionParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return AttentionParams(self.name,self.w_k**factor,self.w_q**factor,self.w_v**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.w_k,self.w_q,self.w_v,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "711444b0-84c7-4af1-a136-3e49310b2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class MultiHeadAttentionParams:\n",
    "    def __init__(self,name,weights,heads:list[AttentionParams]):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"Wo\",weights)\n",
    "        self.heads = heads\n",
    "        self.num_heads = len(heads)\n",
    "    def add(self,head1,head2):\n",
    "        return head1+head2\n",
    "    def subtract(self,head1,head2):\n",
    "        return head1-head2\n",
    "    def multiply(self,val,head):\n",
    "        return val*head\n",
    "    def divide(self,head,val):\n",
    "        return head/val\n",
    "    def pow(self,val,head):\n",
    "        return head**val\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams):\n",
    "            heads = list(map(self.subtract,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights-other.weights,heads)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'MultiHeadAttentionParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams) :\n",
    "            heads = list(map(self.add,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights+other.weights,heads)\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.add,self.heads,[other]*self.num_heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights+other,heads)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'MultiHeadAttentionParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.multiply,[other]*self.num_heads,self.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights*other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams) :\n",
    "            heads = list(map(self.divide,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights/other.weights,heads)\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.divide,self.heads,[other]*self.num_heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights/other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.multiply,[other]*self.num_heads,self.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights*other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        heads = list(map(self.pow,[factor]*self.num_heads,self.heads))\n",
    "        return MultiHeadAttentionParams(self.name,self.weights**factor,heads)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,self.heads,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5101d899-f9b7-4f86-b980-972ce088b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class ModuleParams:\n",
    "    def __init__(self,name,components):\n",
    "        self.name = name\n",
    "        self.components = components\n",
    "        self.num_comps = len(components)\n",
    "    def multiply(self,val,comp):\n",
    "        return val*comp\n",
    "    def subtract(self,comp1,comp2):\n",
    "        return comp1-comp2\n",
    "    def add(self,comp1,comp2):\n",
    "        return comp1+comp2\n",
    "    def pow(self,val,comp):\n",
    "        return comp**val\n",
    "    def divide(self,comp,val):\n",
    "        return comp/val\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,ModuleParams):\n",
    "            comps = list(map(self.subtract,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'ModuleParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,ModuleParams) :\n",
    "            comps = list(map(self.add,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.add,self.components,[other]*self.num_comps))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'ModuleParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.multiply,[other]*self.num_comps,self.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot multiply a 'ModuleParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,ModuleParams) :\n",
    "            comps = list(map(self.divide,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.divide,self.components,[other]*self.num_comps))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot divide a 'ModuleParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.multiply,[other]*self.num_comps,self.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot multiply a 'ModuleParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        comps = list(map(self.pow,[factor]*self.num_comps,self.components))\n",
    "        return ModuleParams(self.name,comps)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.components,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ee510652-2fcb-4349-a7e4-591b8ff7e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "class Dropout:\n",
    "    def __init__(self,dropout_p,seed=0):\n",
    "        self.dropout_p = 0.2\n",
    "        self.seed = seed\n",
    "    def predict(self,x):\n",
    "        _,key_ = random.split(random.key(rnd.randint(0,1000)))\n",
    "        mask_ = random.bernoulli(key_,1-self.dropout_p,shape=x.shape)\n",
    "        dropout_out = mask_*x\n",
    "        scale = 1/(1-self.dropout_p)\n",
    "        return dropout_out*scale\n",
    "    def batched_predict(self,x):\n",
    "        predictor = vmap(self.predict,in_axes=(0))\n",
    "        return predictor(x)\n",
    "    def __call__(self,x):\n",
    "        if len(x.shape)>1:\n",
    "            return self.batched_predict(x)\n",
    "        return self.predict(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3bf926c2-f341-447e-8e5c-774a2a4f592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class LinearLayer:\n",
    "    @classmethod\n",
    "    def initiate_params(cls,name,in_units,out_units,key,scale=1e-2):\n",
    "        w_key,_= random.split(key,2)\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        params = {}\n",
    "        #params[\"W\"] = random.normal(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params[\"W\"] = initializer(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        params = LinearParams(name,initializer(w_key,shape = (in_units,out_units),dtype=jnp.float32)*scale)\n",
    "        return params\n",
    "    def __init__(self,name,in_units,out_units,params=None):\n",
    "        self.in_units = in_units\n",
    "        self.out_units = out_units\n",
    "        self.params = params\n",
    "        self.key = random.key(210)\n",
    "        if params==None:\n",
    "            self.params = LinearLayer.initiate_params(name,self.n_vocab,self.embedding_dims,self.key)\n",
    "    def predict(self,x):\n",
    "        x = jnp.matmul(x,self.params.weights.value)\n",
    "        return x\n",
    "    def batched_predict(self,x):\n",
    "        predictor = vmap(self.predict,in_axes=[0])\n",
    "        return predictor(x)\n",
    "    def __call__(self,x):\n",
    "        if len(x.shape)>2:\n",
    "            return self.batched_predict(x)\n",
    "        return self.predict(x)\n",
    "        #print(x)\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.in_units,self.out_units)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64038d4-be11-4308-ad36-28fe21c29b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf64d75-a164-4ea6-ba6d-17890890c02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "4c3a9fac-3d13-4be9-928a-1bc9c4c09b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LoraLayer:\n",
    "#     def __init__(self,\n",
    "#                 r:int,\n",
    "#                 lora_alpha: int,\n",
    "#                 lora_dropout: int,\n",
    "#                 merge_weights: bool ):\n",
    "#         self.r = r\n",
    "#         self.lora_alpha = lora_alpha\n",
    "#         # Optional dropout\n",
    "#         if lora_dropout > 0.:\n",
    "#             self.lora_dropout = Dropout(p=lora_dropout)\n",
    "#         else:\n",
    "#             self.lora_dropout = lambda x: x\n",
    "#         # Mark the weight as unmerged\n",
    "#         self.merged = False\n",
    "#         self.merge_weights = merge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9676c47a-95c6-4a1c-8676-98d822ee432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class EmbeddingLayer:\n",
    "    @classmethod\n",
    "    def initiate_params(cls,name,n_vocab,embedding_dims,key,scale=1e-1):\n",
    "        w_key,_= random.split(key,2)\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        params = {}\n",
    "        #params[\"W\"] = random.normal(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params[\"W\"] = initializer(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        params = LinearParams(name,initializer(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale)\n",
    "        return params   \n",
    "    @classmethod\n",
    "    def positional_enc(cls,emb_dims,seq_len):\n",
    "        pos = jnp.arange(seq_len)[:, jnp.newaxis]\n",
    "        pe = jnp.zeros((seq_len,emb_dims))\n",
    "        div_terms = jnp.exp(jnp.arange(0, emb_dims, 2) * -(jnp.log(10000.0) / emb_dims))\n",
    "        pe = pe.at[:, 0::2].set(jnp.sin(pos*div_terms))\n",
    "        pe = pe.at[:, 1::2].set(jnp.cos(pos*div_terms))\n",
    "        return pe\n",
    "    def one_hot(self,x,max):\n",
    "        return jnp.array(x[:,:,None]==jnp.arange(max),dtype=jnp.float32)\n",
    "    def __init__(self,name,n_vocab,embedding_dims,params=None):\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.params = params\n",
    "        self.key = random.key(210)\n",
    "        if params==None:\n",
    "            self.params = EmbeddingLayer.initiate_params(name,self.n_vocab,self.embedding_dims,self.key)\n",
    "    def predict(self,x,mask):\n",
    "        seq_len = x.shape[-1]\n",
    "        x = self.one_hot(x,self.n_vocab)\n",
    "        x = jnp.matmul(x,self.params.weights.value)+EmbeddingLayer.positional_enc(self.embedding_dims,seq_len)\n",
    "        mask = jnp.expand_dims(mask,axis=-1)\n",
    "        x=x*mask+jnp.ones(shape=mask.shape)*1e-12\n",
    "        return x\n",
    "    def batched_predict(self,x,mask):\n",
    "        predictor = vmap(self.predict,in_axes=[0,0])\n",
    "        return predictor(x)\n",
    "    def __call__(self,x,mask):\n",
    "        if len(x.shape)>2:\n",
    "            return self.batched_predict(x,mask)\n",
    "        return self.predict(x,mask)\n",
    "        #print(x)\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.n_vocab,self.embedding_dims)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd998bf-1736-40b6-92a3-95cc6b95026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = EmbeddingLayer(name=\"Test_Emb\",n_vocab=1000,embedding_dims=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d5a428-99d2-4a17-a99d-112170befa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.randint(random.key(21),shape=(32,128),minval=0,maxval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cdc009a-7ccf-4d16-a28e-0bbfa75e2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = jnp.ones(shape=(32,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9d15e-1be0-4471-9248-020f58de7613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "274785a8-1694-4583-aa62-859d707349bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jnp.matmul(emb.params.weights.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f9f5d6d-b749-4aa7-8602-9bf34ed3afc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-5.3115553e-01,  8.4907073e-01, -8.3078086e-01, ...,\n",
       "         9.9777108e-01,  4.4762911e-03,  9.9656099e-01],\n",
       "       [-5.3290927e-01,  8.4754169e-01, -8.2782418e-01, ...,\n",
       "         1.0035292e+00,  1.0806098e-02,  1.0036786e+00],\n",
       "       [-5.3966665e-01,  8.4262085e-01, -8.4065729e-01, ...,\n",
       "         9.9260503e-01, -1.3450127e-03,  9.9290007e-01],\n",
       "       ...,\n",
       "       [-5.3454167e-01,  8.4251159e-01, -8.3225435e-01, ...,\n",
       "         9.9481994e-01,  3.6581811e-03,  9.9596053e-01],\n",
       "       [-5.3558612e-01,  8.3551776e-01, -8.3124691e-01, ...,\n",
       "         9.9556798e-01, -4.6265158e-03,  1.0012770e+00],\n",
       "       [-5.3675395e-01,  8.3664453e-01, -8.2822013e-01, ...,\n",
       "         1.0007498e+00, -1.2522831e-04,  9.9827188e-01]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(x,mask)[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d4e41b-867f-42b1-b4af-ef37cef877b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LoraEmbedding(EmbeddingLayer,LoraLayer):\n",
    "#      def __init__(self,\n",
    "#                 r:int,\n",
    "#                 lora_alpha: int,\n",
    "#                 lora_dropout: int,\n",
    "#                 merge_weights: bool ,\n",
    "#                 n_vocab:int,\n",
    "#                 embedding_dims:int,\n",
    "#                 params = None):\n",
    "#          self.key = random.key(5)\n",
    "#          EmbeddingLayer.__init__(n_vocab,embedding_dims,params)\n",
    "#          LoraLayer.__init__(r,lora_alpha,lora_dropout,merge_weights)\n",
    "#          self.A =  random.normal(self.key,shape=(r,n_embeddings)\n",
    "#          self.B = jnp.zeros(shape = (n_vocab,r))\n",
    "#     def __call__(self,x):\n",
    "#         x = self.one_hot(x,self.n_vocab)\n",
    "#         W = jnp.dot(x,self.params)\n",
    "#         delta_W = jnp.dot(B,A)\n",
    "        \n",
    "#         return  + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41eabfe4-4442-4790-aecc-4ee0f276be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestNetwork:\n",
    "#     def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f1c2cff-6420-4a2d-ace6-abddf9a70ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "738a31eb-590a-43a7-b3c5-0a023ca651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class FeedForward:\n",
    "    def initiate_params(name,input_shape,units,key,scale=1e-4):\n",
    "        w_key,b_key = random.split(key,2)\n",
    "        params = {}\n",
    "        #params[\"W\"] = random.normal(w_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        #params[\"b\"] = random.normal(b_key,shape = (units,))*scale\n",
    "        \n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params[\"W\"] = initializer(w_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        params = FeedForwardParams(name,\n",
    "                                   weights = initializer(w_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                   bias = random.normal(b_key,shape = (units,))*scale)\n",
    "        #params[\"b\"] = initializer(b_key,shape = (units,))*scale\n",
    "        return params\n",
    "    def __init__(self,name,d_model,units,activation=lambda x:x,params=None):\n",
    "        self.activation = activation\n",
    "        self.units = units\n",
    "        self.key = random.key(210)\n",
    "        self.d_model = d_model\n",
    "        if params == None:\n",
    "            self.params = FeedForward.initiate_params(name,d_model,self.units,self.key)\n",
    "        else:\n",
    "            self.params = params\n",
    "    def predict(self,input):\n",
    "        return self.activation(jnp.matmul(input,self.params.weights.value)+self.params.bias.value)\n",
    "    def batched_predict(self,inputs):\n",
    "        predictor = vmap(self.predict,in_axes = (0))\n",
    "        return predictor(inputs)\n",
    "    def __call__(self,input):\n",
    "        if len(input.shape)>1:\n",
    "            return self.batched_predict(input)\n",
    "        return self.predict(input)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.units,self.activation)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4407feaa-19e8-4390-8786-ad8846e5198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(name=\"ff1\",d_model=512,units=2048,activation=relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6c88d29-e84f-485e-8956-89b64fed33df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.5880790e-04, 3.9633096e-04, 2.1654676e-04, ..., 2.3233908e-05,\n",
       "        0.0000000e+00, 3.7421738e-05],\n",
       "       [1.5861183e-04, 3.9658550e-04, 2.1635339e-04, ..., 2.2884618e-05,\n",
       "        0.0000000e+00, 3.6784462e-05],\n",
       "       [1.5878624e-04, 3.9760862e-04, 2.1506406e-04, ..., 2.2920343e-05,\n",
       "        0.0000000e+00, 3.6622201e-05],\n",
       "       ...,\n",
       "       [1.5958722e-04, 3.9776997e-04, 2.1453734e-04, ..., 2.2507738e-05,\n",
       "        0.0000000e+00, 3.7097256e-05],\n",
       "       [1.5870271e-04, 3.9681490e-04, 2.1639798e-04, ..., 2.2913002e-05,\n",
       "        0.0000000e+00, 3.8226081e-05],\n",
       "       [1.5863587e-04, 3.9581870e-04, 2.1496273e-04, ..., 2.3156172e-05,\n",
       "        0.0000000e+00, 3.7206482e-05]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff(emb(x,mask))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43ececb6-534d-476e-b0ca-03228abe9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class AttentionHead:\n",
    "    def initiate_params(name,input_shape,units,key,scale=1e-2):\n",
    "        q_key,k_key,v_key = random.split(key,3)\n",
    "        \n",
    "        params = {}\n",
    "        # params[\"Wq\"] = random.normal(q_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params[\"Wk\"] = random.normal(k_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params['Wv'] = random.normal(v_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        # params[\"Wq\"] = initializer(q_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params[\"Wk\"] = initializer(k_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params['Wv'] = initializer(v_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        params = AttentionParams(name=name,\n",
    "                                 w_q = initializer(q_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                 w_k = initializer(k_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                 w_v = initializer(v_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "                                )\n",
    "                                 \n",
    "        return params\n",
    "    def __init__(self,name,d,d_model,params=None):\n",
    "        self.d = d\n",
    "        self.d_model = d_model \n",
    "        self.key = random.key(210)\n",
    "        self.params = params\n",
    "        if params ==None:\n",
    "            self.params = AttentionHead.initiate_params(name,self.d_model,self.d,self.key)\n",
    "    def predict(self,x_q,x_k,x_v,mask,decoder=False):\n",
    "        query = jnp.matmul(x_q,self.params.w_q.value)\n",
    "        key = jnp.matmul(x_k,self.params.w_k.value)\n",
    "        value = jnp.matmul(x_v,self.params.w_v.value)\n",
    "        #print(\"Attenion Shapes:\",query.shape,key.shape,value.shape)\n",
    "        attn_scores = jnp.matmul(query,key.T)/jnp.sqrt(self.d)\n",
    "        if mask != None:\n",
    "            mask = jnp.expand_dims(mask,axis=0)\n",
    "            #print(mask*mask.T)\n",
    "            attn_scores = attn_scores*(mask*mask.T) +(mask*mask.T!=1)*(-1e-20)\n",
    "        #print(attn_scores)\n",
    "        #print(attn_scores.shape)\n",
    "        softmaxed_attn = jax.nn.softmax(attn_scores)\n",
    "        if mask != None:\n",
    "            softmaxed_attn = softmaxed_attn*(mask*mask.T) +(mask*mask.T!=1)*(1e-32)\n",
    "        #softmaxed_attn = jnp.nan_to_num(softmaxed_attn)\n",
    "        #print(softmaxed_attn)\n",
    "        if decoder:\n",
    "            softmaxed_attn = softmaxed_attn*jnp.triu(jnp.ones(attn_scores.shape))\n",
    "        #print(softmaxed_attn)\n",
    "        #print(\"Value Matrix:\",value.shape)\n",
    "        return jnp.matmul(softmaxed_attn,value)\n",
    "    def batched_predict(self,x_q,x_k,x_v,mask,decoder=False):\n",
    "        predictor = vmap(self.predict,in_axes = (0,0,0,0,None))\n",
    "        return predictor(x_q,x_k,x_v,mask,decoder)\n",
    "    def __call__(self,x_q,x_k,x_v,mask,decoder=False):\n",
    "        if len(x_q.shape)>1:\n",
    "            #print(x_q.shape)\n",
    "            return self.batched_predict(x_q,x_k,x_v,mask,decoder)\n",
    "        return self.predict(x_q,x_k,x_v,mask,decoder)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d,self.d_model)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7d4b7-f5fe-4053-9b0b-6900e66027c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddd21ca5-f990-429c-a66a-730a34eeb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class MultiHeadAttention:\n",
    "    def initiate_params(name,num_heads,input_shape,units,key,scale=1e-3):\n",
    "        o_key,*h_key = random.split(key,num_heads+1)\n",
    "        #print(o_key,h_key)\n",
    "        params = {}\n",
    "        #params['Wo'] = random.normal(o_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params['Wo'] = initializer(o_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        params = MultiHeadAttentionParams(name,\n",
    "                                          weights = initializer(o_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                          heads = [AttentionHead.initiate_params(f\"H{i}\",\n",
    "                                                                                 input_shape,\n",
    "                                                                                 input_shape//num_heads,\n",
    "                                                                                 h_key[i]) \n",
    "                                                   for i in range(num_heads)\n",
    "                                                  ]\n",
    "                                         )\n",
    "        return params\n",
    "    def __init__(self,name,h,d_model,params=None):\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.key = random.key(210)\n",
    "        self.d = d_model//h\n",
    "        self.params = params\n",
    "        if params ==None:\n",
    "            self.params = MultiHeadAttention.initiate_params(name,self.h,self.d_model,self.d_model,self.key)\n",
    "        \n",
    "        if self.d_model%self.h!=0:\n",
    "            raise \"D_model not divisible by number of heads\"\n",
    "        self.attentionHeads = [AttentionHead(f\"H{i}\",\n",
    "                                             self.d,\n",
    "                                             self.d_model,\n",
    "                                             self.params.heads[i]) \n",
    "                               for i in range(self.h)]\n",
    "        \n",
    "    #def predict(self,x_q,x_k,x_v,mask=None,decoder=False):\n",
    "        #return jnp.matmul(self.params['Wo'],jnp.concat([head.predict(x_q_i,x_k_i,x_v_i,mask,decoder) for head,x_q_i,x_k_i,x_v_i in zip(self.attentionHeads,[x_q]*8,[x_k]*8,[x_v]*8)]))\n",
    "    def calc_attentions(self,x_q,x_k,x_v,mask=None,decoder=False):\n",
    "        concat_attn = jnp.concat([head.batched_predict(x_q_i,x_k_i,x_v_i,mask,decoder) \n",
    "                                  for head,x_q_i,x_k_i,x_v_i in \n",
    "                                  zip(self.attentionHeads,[x_q]*8,[x_k]*8,[x_v]*8)],\n",
    "                                 axis=-1)\n",
    "        return concat_attn\n",
    "    def predict(self,attns):\n",
    "        return jnp.matmul(attns,self.params.weights.value)\n",
    "    def batched_predict(self,attns):\n",
    "        predictor = vmap(self.predict,in_axes = (0))\n",
    "        return predictor(attns)\n",
    "    def __call__(self,x_q,x_k,x_v,mask=None,decoder=False):\n",
    "        attns = self.calc_attentions(x_q,x_k,x_v,mask,decoder)\n",
    "        if len(x_q.shape)>1: \n",
    "            return self.batched_predict(attns)\n",
    "        return self.predict(attns)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.h,self.d_model)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "211c7f53-8ee0-49a9-90a4-64f83578d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class EncoderLayer:\n",
    "    def layer_normalization(output,epsilon=1e-9):\n",
    "        H = output.shape[-1]\n",
    "        mean = jnp.expand_dims(output.mean(axis=-1),axis=-1)\n",
    "        std = jnp.expand_dims(output.std(axis=-1),axis=-1)\n",
    "        output = (output - mean)/(std+epsilon)\n",
    "        return output\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,params=None):\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.params = params\n",
    "        self.key = random.key(210)\n",
    "        if params==None:\n",
    "            ff1_key,ff2_key,mha_key = random.split(self.key,3)\n",
    "            self.params = ModuleParams(name,\n",
    "                                       [MultiHeadAttention.initiate_params('mha',num_heads,d_model,d_model,mha_key),\n",
    "                                        FeedForward.initiate_params('ff1',d_model,d_ff,ff1_key),\n",
    "                                        FeedForward.initiate_params('ff2',d_ff,d_model,ff2_key)])\n",
    "        \n",
    "        self.ff1 = FeedForward(\"ff1\",d_model,d_ff,params=self.params.components[1])\n",
    "        self.ff2 = FeedForward(\"ff2\",d_ff,d_model,params=self.params.components[2])\n",
    "        #self.__name__ = f\"EncoderLayer{num}\"\n",
    "        self.mha = MultiHeadAttention(\"mha\",num_heads,d_model,self.params.components[0])\n",
    "        self.dropout = Dropout(0.2)\n",
    "       \n",
    "        #print(self.params)\n",
    "    def predict(self,input,mask):\n",
    "        attentions = self.mha(input,input,input,mask)\n",
    "        #print(mask)\n",
    "        attentions = self.dropout(attentions)\n",
    "        #print(\"Attentions\")\n",
    "        #print(attentions)\n",
    "        x = EncoderLayer.layer_normalization(input+attentions)\n",
    "        #print(\"x+attentions\")\n",
    "       # print(x)\n",
    "        ff_ = self.ff2(self.ff1(x))\n",
    "        ff_ = self.dropout(ff_)\n",
    "        #print(\"x+ff_\")\n",
    "        #print(x+ff_)\n",
    "        x = EncoderLayer.layer_normalization(x+ff_)\n",
    "        #print(x)\n",
    "        return x\n",
    "    # def batched_predict(self,inputs,mask):\n",
    "    #     predictor = vmap(self.predict,in_axes=(0,0))\n",
    "    #     return predictor(inputs,mask)\n",
    "    def __call__(self,inputs,mask):\n",
    "        # if len(inputs.shape)>1:\n",
    "        #     return self.batched_predict(inputs,mask)\n",
    "        return self.predict(inputs,mask)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7016ac8-6054-4978-b103-477dfa190f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EncoderLayer.layer_normalization(random.normal(random.key(210),shape=(64,128,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2acdb622-838c-46ed-a926-2b059b9703b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayerParams:\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,key):\n",
    "        ff1_key,ff2_key,mha_key = random.split(key,3)\n",
    "        self.params = ModuleParams(name,\n",
    "                                    [MultiHeadAttention.initiate_params('mha',num_heads,d_model,d_model,mha_key),\n",
    "                                    FeedForward.initiate_params('ff1',d_model,d_ff,ff1_key),\n",
    "                                    FeedForward.initiate_params('ff2',d_ff,d_model,ff2_key)])\n",
    "        \n",
    "\n",
    "class EncoderParams:\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,num_layers,key):\n",
    "        keys = random.split(key,num_layers)\n",
    "        self.params = ModuleParams(name,\n",
    "                                   [EncoderLayerParams(f\"L{i}\",d_model,d_ff,num_heads,key_).params \n",
    "                                    for i,key_ in enumerate(keys)])\n",
    "        \n",
    "@register_pytree_node_class\n",
    "class Encoder:\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,num_layers,params=None):\n",
    "        #self.num= generate_number(num_layers)\n",
    "        self.d_model=d_model\n",
    "        self.d_ff=d_ff\n",
    "        self.num_heads=num_heads\n",
    "        self.num_layers=num_layers\n",
    "        self.key = random.key(210)\n",
    "        self.params = params\n",
    "        if params==None:\n",
    "            self.params = ModuleParams(name,\n",
    "                                   [EncoderLayerParams(f\"L{i}\",d_model,d_ff,num_heads,key_).params \n",
    "                                    for i,key_ in enumerate(keys)])\n",
    "        self.layers = [EncoderLayer(f\"L{i}\",d_model,d_ff,num_heads,self.params.components[i]) for i in range(num_layers)]\n",
    "    def __call__(self,input,mask):\n",
    "        x = input\n",
    "        #print(\"Encoder Input Shape:\",x.shape)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return x\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads,self.num_layers)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54a4b4a7-cbf0-403e-b54d-23166499bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayerParams:\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,key):\n",
    "        self.params = {}\n",
    "        ff1_key,ff2_key,e_mha_key,d_mha_key = random.split(key,4)\n",
    "        self.params = ModuleParams(name,\n",
    "                                [MultiHeadAttention.initiate_params('d_mha',num_heads,d_model,d_model,d_mha_key),\n",
    "                                 MultiHeadAttention.initiate_params('e_mha',num_heads,d_model,d_model,e_mha_key),\n",
    "                                 FeedForward.initiate_params('ff1',d_model,d_ff,ff1_key),\n",
    "                                 FeedForward.initiate_params('ff2',d_ff,d_model,ff2_key)])\n",
    "\n",
    "@register_pytree_node_class        \n",
    "class DecoderLayer:\n",
    "    def layer_normalization(output,epsilon=1e-9):\n",
    "        H = output.shape[-1]\n",
    "        mean = jnp.expand_dims(output.mean(axis=-1),axis=-1)\n",
    "        std = jnp.expand_dims(output.std(axis=-1),axis=-1)\n",
    "        output = (output - mean)/(std+epsilon)\n",
    "        return output\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,params=None):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.key = random.key(210)\n",
    "        self.params = params\n",
    "        if params ==None:\n",
    "            ff1_key,ff2_key,e_mha_key,d_mha_key = random.split(self.key,4)\n",
    "            self.params = ModuleParams(name,\n",
    "                                    [MultiHeadAttention.initiate_params('d_mha',num_heads,d_model,d_model,d_mha_key),\n",
    "                                     MultiHeadAttention.initiate_params('e_mha',num_heads,d_model,d_model,e_mha_key),\n",
    "                                     FeedForward.initiate_params('ff1',d_model,d_ff,ff1_key),\n",
    "                                     FeedForward.initiate_params('ff2',d_ff,d_model,ff2_key)])\n",
    "        self.ff1 = FeedForward(\"ff1\",d_model,d_ff,params = self.params.components[2])\n",
    "        self.ff2 = FeedForward(\"ff2\",d_ff,d_model,params = self.params.components[3])\n",
    "        self.d_mha = MultiHeadAttention(\"d_mha\",num_heads,d_model,params = self.params.components[0])\n",
    "        self.e_mha = MultiHeadAttention(\"e_mha\",num_heads,d_model,params = self.params.components[1])\n",
    "        self.dropout = Dropout(0.2)\n",
    "    def predict(self,input,encoder_output,mask):\n",
    "        attentions = self.d_mha(input,input,input,mask,decoder=True)\n",
    "        #attentions = self.dropout(attentions)\n",
    "        x = DecoderLayer.layer_normalization(input+attentions)\n",
    "        e_attentions = self.e_mha(x,encoder_output,encoder_output,mask)\n",
    "        #e_attentions = self.dropout(e_attentions)\n",
    "        x = DecoderLayer.layer_normalization(x+e_attentions)\n",
    "        ff_ = self.ff2(self.ff1(x))\n",
    "        ff_ = self.dropout(ff_)\n",
    "        x = DecoderLayer.layer_normalization(x+ff_)\n",
    "        return x\n",
    "    # def batched_predict(self,inputs,encoder_output,mask):\n",
    "    #     predictor = vmap(self.predict,in_axes=(0,0,0))\n",
    "    #     return predictor(inputs,encoder_output,mask)\n",
    "    def __call__(self,inputs,encoder_output,mask):\n",
    "        # if len(inputs.shape)>1:\n",
    "        #     return self.batched_predict(inputs,encoder_output)\n",
    "        return self.predict(inputs,encoder_output,mask)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be858598-5914-4380-9f5f-76b33173b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderParams:\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,num_layers,key):\n",
    "        keys = random.split(key,num_layers)\n",
    "        self.params = ModuleParams(name,\n",
    "                                   [DecoderLayerParams(f\"L{i}\",d_model,d_ff,num_heads,key_).params \n",
    "                                    for i,key_ in enumerate(keys)])\n",
    "@register_pytree_node_class\n",
    "class Decoder:\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,num_layers,params=None):\n",
    "        self.params = params\n",
    "        self.keys = random.split(random.key(251),num_layers)\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        if params==None:\n",
    "            self.params = ModuleParams(name,\n",
    "                                   [DecoderLayerParams(f\"L{i}\",d_model,d_ff,num_heads,key_).params \n",
    "                                    for i,key_ in enumerate(keys)])\n",
    "        self.layers = [DecoderLayer(f\"L{i}\",d_model,d_ff,num_heads,self.params.components[i]) for i in range(num_layers)]\n",
    "    def __call__(self,input,encoder_output,mask):\n",
    "        x = input\n",
    "        #print(mask)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output,mask)\n",
    "        return x\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads,self.num_layers)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "178f068b-18b9-4363-a81c-c02a9bdfc2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mLinearLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_1931/2505762081.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?LinearLayer.initiate_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a1da60a8-2c54-4285-abd2-1e46a1dd179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerParams:\n",
    "    def __init__(self,d_model,d_ff,num_heads,num_layers,n_vocab,key):\n",
    "        self.params = {}\n",
    "        key_emb,key_emb_dec,key_e,key_d,key_l = random.split(key,5)\n",
    "        #self.params[\"Embedding\"] = EmbeddingLayer.initiate_params(n_vocab,d_model,key_emb)\n",
    "        self.params   = ModuleParams(\"Transformer\",\n",
    "                                     [EmbeddingLayer.initiate_params(\"In_Embedding\",n_vocab,d_model,key_emb),\n",
    "                                     EmbeddingLayer.initiate_params(\"Out_Embedding\",n_vocab,d_model,key_emb_dec),\n",
    "                                     EncoderParams(\"Encoder\",d_model,d_ff,num_heads,num_layers,key_e).params,\n",
    "                                     DecoderParams(\"Decoder\",d_model,d_ff,num_heads,num_layers,key_d).params,\n",
    "                                     LinearLayer.initiate_params(\"Linear\",d_model,n_vocab,key_l)])\n",
    "        \n",
    "\n",
    "@register_pytree_node_class\n",
    "class Transformer:\n",
    "    def __init__(self,d_model,d_ff,num_heads,num_layers,n_vocab,logits=False,params=None,seed=0):\n",
    "        self.d_model = d_model\n",
    "        self.d_ff=d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.n_vocab = n_vocab\n",
    "        self.logits = logits\n",
    "        self.params = params\n",
    "        self.key = random.key(seed)\n",
    "        if params == None:\n",
    "            self.params = TransformerParams(d_model,d_ff,num_heads,num_layers,n_vocab,self.key).params\n",
    "        #self.embedding = EmbeddingLayer(n_vocab,d_model,self.params[\"Embedding\"])\n",
    "        self.in_embedding = EmbeddingLayer(\"In_Embedding\",n_vocab,d_model,self.params.components[0])\n",
    "        self.out_embedding = EmbeddingLayer(\"Out_Embedding\",n_vocab,d_model,self.params.components[1])\n",
    "        self.encoder = Encoder(\"Encoder\",d_model,d_ff,num_heads,num_layers,self.params.components[2])\n",
    "        self.decoder = Decoder(\"Decoder\",d_model,d_ff,num_heads,num_layers,self.params.components[3])\n",
    "        self.linear = LinearLayer(\"Linear\",d_model,n_vocab,params=self.params.components[4])\n",
    "        self.logits = logits\n",
    "    def update_params(self):\n",
    "        self.in_embedding = EmbeddingLayer(\"In_Embedding\",self.n_vocab,self.d_model,self.params.components[0])\n",
    "        self.out_embedding = EmbeddingLayer(\"Out_Embedding\",self.n_vocab,self.d_model,self.params.components[1])\n",
    "        self.encoder = Encoder(\"Encoder\",self.d_model,self.d_ff,self.num_heads,self.num_layers,self.params.components[2])\n",
    "        self.decoder = Decoder(\"Decoder\",self.d_model,self.d_ff,self.num_heads,self.num_layers,self.params.components[3])\n",
    "        self.linear = LinearLayer(\"Linear\",self.d_model,self.n_vocab,params=self.params.components[4])\n",
    "    def __call__(self,inputs,outputs):\n",
    "        input_tokens = jnp.array(inputs['token_ids'])\n",
    "        input_mask = jnp.array(inputs['padding_mask'])\n",
    "        output_tokens = jnp.array(outputs['token_ids'])\n",
    "        output_mask = jnp.array(outputs['padding_mask'])\n",
    "        input_tokens = Padder.left_shift(input_tokens,5)\n",
    "        input_mask = Padder.left_shift_mask(input_mask)\n",
    "        embs = self.in_embedding(input_tokens,input_mask)\n",
    "        #print(\"In_Embeddings\",embs)\n",
    "        op_embs = self.out_embedding(output_tokens,output_mask)\n",
    "        #print(\"Out_Embeddings\",op_embs)\n",
    "        if len(embs.shape)!=3:\n",
    "            raise \"Dimensions of the input must include (Batch,Token Sequence)\"\n",
    "        encoder_output = self.encoder(embs,input_mask)\n",
    "        #print(\"Encoder Output:\",encoder_output)\n",
    "        decoder_output = self.decoder(op_embs,encoder_output,output_mask)\n",
    "        #print(\"Decoder Output:\",decoder_output)\n",
    "        output = self.linear(decoder_output)\n",
    "        #print(\"Linear Output:\",output)\n",
    "        if self.logits:\n",
    "            return output\n",
    "        return jax.nn.softmax(output,axis=-1)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads,self.num_layers,self.n_vocab,self.logits)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faaf70e4-1308-443c-8d0d-90e7deadfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Optimizer:\n",
    "#     def __init__(self,lr,lambda_):\n",
    "#         self.lr = lr\n",
    "#         self.lambda_ = lambda_\n",
    "#     def update_coder_params(self,t,params,grads):\n",
    "#         for layer in params:\n",
    "#             params[layer] = self.update_layer_params(t,\n",
    "#                 params[layer],\n",
    "#                 grads[layer]\n",
    "#             )\n",
    "#         return params\n",
    "#     def update_layer_params(self,t,params,grads):\n",
    "#         for type in params:\n",
    "#             if 'ff' in type:\n",
    "#                 params[type] = self.update_basic_params(t,\n",
    "#                     params[type],\n",
    "#                     grads[type]\n",
    "#                 )\n",
    "#             if 'mha' in type:\n",
    "#                 params[type] = self.update_mha_params(t,\n",
    "#                     params[type],\n",
    "#                     grads[type]\n",
    "#                 )\n",
    "#         return params\n",
    "#     def update_mha_params(self,t,params,grads):\n",
    "#         params['Wo'] = self.update_params(t,params['Wo'],grads['Wo'])\n",
    "#         for head in params:\n",
    "#             if head == 'Wo':\n",
    "#                 continue\n",
    "#             for type in params[head]:\n",
    "#                 #print(head,type)\n",
    "#                 params[head][type] = self.update_params(t,params[head][type],grads[head][type])\n",
    "#         return params\n",
    "#     def update_basic_params(self,t,params,grads):\n",
    "#         for type in params:\n",
    "#             params[type]= self.update_params(t,params[type],grads[type])\n",
    "#         return params\n",
    "#     def update_params(self,t,params,grads):\n",
    "#         params = params - self.lr*(grads)\n",
    "#         return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d48c2b33-810d-4123-97a4-9391b184e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self,lr,lambda_):\n",
    "        self.lr = lr\n",
    "        self.lambda_ = lambda_\n",
    "    def update_params(self,t,params,grads):\n",
    "        params = params - self.lr*grads\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "22b80582-1f67-4750-9e2c-c82ec4ed5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    '''Implementation of AdamW \n",
    "    Optimizer using JAX'''\n",
    "    def __init__(self,lr,beta1,beta2,epsilon,lambda_):\n",
    "        super().__init__(lr,lambda_)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "    def SetScheduleMultiplier(self,t):\n",
    "        return 0.0001\n",
    "    def update_params(self,t,params,grads):\n",
    "        g = grads + self.lambda_*params\n",
    "        if not self.m:\n",
    "            self.m = (1-self.beta1)*g\n",
    "            self.v = (1-self.beta2)*(g**2)\n",
    "        else:\n",
    "            self.m = self.beta1*self.m + (1-self.beta1)*g\n",
    "            self.v = self.beta2*self.v + (1-self.beta2)*(g**2)\n",
    "        m_hat = self.m/(1-(self.beta1)**t)\n",
    "        v_hat = self.v/(1-(self.beta2)**t)\n",
    "        eta = self.SetScheduleMultiplier(t)\n",
    "        params = params - eta*((self.lr*m_hat)/((v_hat)**0.5+self.epsilon) + self.lambda_*params)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "88495ac4-79f4-4dcd-85db-39bfaa778a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,model,loss,optimizer,schedular=None):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        #self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.schedular = None\n",
    "        if schedular!=None:\n",
    "            self.schedular = schedular(optimizer)\n",
    "    # def update(self,t,grads):\n",
    "    #     self.model.params[\"Encoder\"] = self.optimizer.update_coder_params(t,self.model.params[\"Encoder\"],grads[\"Encoder\"])\n",
    "    #     self.model.params[\"Decoder\"] = self.optimizer.update_coder_params(t,self.model.params[\"Decoder\"],grads[\"Decoder\"])\n",
    "    #     self.model.params[\"Linear\"] = self.optimizer.update_basic_params(t,self.model.params[\"Linear\"],grads[\"Linear\"])\n",
    "    #     #self.model.params[\"Embedding\"] = self.optimizer.update_basic_params(t,self.model.params[\"Embedding\"],grads[\"Embedding\"])\n",
    "    #     self.model.params[\"In_Embedding\"] = self.optimizer.update_basic_params(t,self.model.params[\"In_Embedding\"],grads[\"In_Embedding\"])\n",
    "    #     self.model.params[\"Out_Embedding\"] = self.optimizer.update_basic_params(t,self.model.params[\"Out_Embedding\"],grads[\"Out_Embedding\"])\n",
    "    def update(self,t,grads):\n",
    "        self.model.params = self.optimizer.update_params(t,self.model.params,grads)\n",
    "        self.model.update_params()\n",
    "    def train(self,x,y,epochs,batch_size=None):\n",
    "        data_size = len(x[0]['token_ids'])\n",
    "        if batch_size== None:\n",
    "            batch_size = data_size\n",
    "        # if learning_rate!=None:\n",
    "        #     self.lr = learning_rate\n",
    "        t = 0\n",
    "        for epoch in range(epochs):\n",
    "            curr = 0\n",
    "            print(\"Epoch \",epoch,\"\\r\",flush=True)\n",
    "            print(\"Loss:\")\n",
    "            while curr<data_size:\n",
    "                t+=1\n",
    "                batch_x = (\n",
    "                    {'token_ids':x[0]['token_ids'][curr:curr+batch_size],\n",
    "                     'padding_mask':x[0]['padding_mask'][curr:curr+batch_size]},\n",
    "                    {'token_ids':x[1]['token_ids'][curr:curr+batch_size],\n",
    "                     'padding_mask':x[1]['padding_mask'][curr:curr+batch_size]}\n",
    "                          )\n",
    "                batch_y = y[curr:curr+batch_size]\n",
    "                #print(Padder.left_shift(jnp.array(batch_y),5))\n",
    "                #break\n",
    "                batch_y = one_hot(Padder.left_shift(jnp.array(batch_y),5),self.model.n_vocab)\n",
    "                curr = curr+batch_size\n",
    "                grads = grad(self.loss)(self.model,batch_x,batch_y).params\n",
    "                #grads = clip_gradients(grads)\n",
    "                self.update(t,grads)\n",
    "                if self.schedular!=None:\n",
    "                    self.schedular.update(512,t,4000)\n",
    "                    self.optimizer = self.schedular.optimizer\n",
    "                    #print(\"Learning Rate:\",self.optimizer.lr)\n",
    "                print(self.loss(self.model,batch_x,batch_y),end=\"\\r\",flush=True)\n",
    "            print(self.loss(self.model,batch_x,batch_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "62eda2a5-1f9f-449d-a033-74a41332ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalCrossEntropy(transformer,x,y):\n",
    "    input_tokens = x[0]\n",
    "    output_tokens = x[1]\n",
    "    y_hat = transformer(input_tokens,output_tokens)\n",
    "    labels = jnp.argmax(y,axis=-1)\n",
    "    mask = labels!=5\n",
    "    return jnp.mean(-(((y*jnp.log(y_hat)).sum(axis=-1))*mask).sum(axis=-1,keepdims=True)/mask.sum(axis=-1,keepdims=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "04d080c7-62b8-49a7-bc6f-0f41b1e5156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ed5d732-157f-42c3-af22-e2114d4d8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CategoricalCrossEntropy(Trans,({'token_ids':en_tokens[:32],'padding_mask':en_mask[:32]},\n",
    "#                {'token_ids':fr_tokens[:32],'padding_mask':fr_mask[:32]}),\n",
    "#               one_hot(Padder.left_shift(jnp.array(fr_tokens[:32]),5),1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7d8c2522-c2b6-4370-8a8e-f401d3801116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trans({'token_ids':en_tokens[:32],'padding_mask':en_mask[:32]},{'token_ids':fr_tokens[:32],'padding_mask':fr_mask[:32]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abc725-a759-43a8-be7c-c99defbe1333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ee35d3e-36fc-4347-85c8-dad918be97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_loss = CategoricalCrossEntropy(Trans,\n",
    "                                    # ({'token_ids':en_tokens[:32],'padding_mask':en_mask[:32]},\n",
    "                                    # {'token_ids':fr_tokens[:32],'padding_mask':fr_mask[:32]}),\n",
    "                                    # one_hot(Padder.left_shift(jnp.array(fr_tokens[:32]),5),1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb32aee-5fd3-4504-beb5-eb1c0b01c9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "312c079e-924f-4867-a45a-c4c6948b6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e52da1ea-1a93-4f79-8d28-73f0d143070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4569f4d8-2746-44f5-9a8f-b95f739f0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x,max):\n",
    "        return jnp.array(x[:,:,None]==jnp.arange(max),dtype=jnp.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24423274-f8c4-4c01-a73c-d07f25dc04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CategoricalCrossEntropy(Trans,({'token_ids':en_tokens,'padding_mask':en_mask},{'token_ids':fr_tokens,'padding_mask':fr_mask}),y_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b3b5de67-4927-41a1-8443-1099b246c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = random.randint(random.key(3),minval=0,maxval=3000,shape = (100,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce626216-1519-4890-a2b3-6cf0fc73770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jnp.argmax(Trans(x,x),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "901a46ca-ca00-4d35-b3aa-f2acf26b3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SubwordTokenizer:\n",
    "    # def __init__():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca279b4a-5c14-4aa8-8af9-34011c682161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tokenizer:\n",
    "#     def __init__(self,vocab_size,):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e96868a-95a8-49a3-958d-8141380f25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unicodedata\n",
    "# import re\n",
    "# sentence = \"I'm going to be some person, I couldn't be what I wanted to be but I'll be someone in 2025\"\n",
    "# # \n",
    "# re.sub(r\"\\s+\",\" \",re.sub(r\"([^\\'\\w])\",r\" \\1\",sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d9468c8-32d0-4da2-9487-1d8855993f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def clean_text(sentence):\n",
    "#     pattern = r'[\\s]+'\n",
    "#     sentence = re.sub(r\"\\s+\",\" \",re.sub(r\"([^\\'\\w])\",r\" \\1\",sentence))\n",
    "#     contractions = {\"'ve\":\" have\",\n",
    "#                     \"'ll\":\" will\",\n",
    "#                     \"'m\":\" am\",\n",
    "#                     \"'re\":\" are\",\n",
    "#                     \"n't\":\" not\",\n",
    "#                     \"'d\":\" had\"}\n",
    "#     # sentence = unicodedata.normalize(\"NFD\",sentence)\n",
    "#     words = re.split(pattern,sentence)\n",
    "#     for contraction in contractions:\n",
    "#         words = [word.replace(contraction,contractions[contraction]) if contraction in word else word for word in words]\n",
    "#     words = re.split(pattern,\" \".join(words))\n",
    "#     return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cae93d0d-2508-4f5b-81c1-b148f808effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6a895605-fcf6-40b0-b1b6-76d9bef6a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.decode([1]+sp.encode(clean_text(sentence))+[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5f2cdf99-5a24-4d40-b0e8-63460bbcbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import re\n",
    "import unicodedata\n",
    "class Tokenizer:\n",
    "    import sentencepiece as spm\n",
    "    def __init__(self,model_prefix):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = self.model_prefix + \".model\"\n",
    "        try:\n",
    "            self.sp = spm.SentencePieceProcessor(model_file = self.model_file)\n",
    "        except:\n",
    "            print(\"Model File Not Found. Tokenizer must be trained in order to make changes.\")\n",
    "    @classmethod\n",
    "    def clean_text(cls,sentence):\n",
    "        pattern = r'[\\s]+'\n",
    "        sentence = re.sub(r\"\\s+\",\" \",re.sub(r\"([^\\'\\w])\",r\" \\1\",sentence))\n",
    "        contractions = {\"'ve\":\" have\",\n",
    "                    \"'ll\":\" will\",\n",
    "                    \"'m\":\" am\",\n",
    "                    \"'re\":\" are\",\n",
    "                    \"n't\":\" not\",\n",
    "                    \"'d\":\" had\"}\n",
    "        sentence = unicodedata.normalize(\"NFD\",sentence)\n",
    "        words = re.split(pattern,sentence)\n",
    "        for contraction in contractions:\n",
    "            words = [word.replace(contraction,contractions[contraction]) if contraction in word else word for word in words]\n",
    "        words = re.split(pattern,\" \".join(words))\n",
    "        return \" \".join(words)\n",
    "    @classmethod\n",
    "    def batched_clean_text(cls,x):\n",
    "        return [text for text in map(cls.clean_text,x)]\n",
    "        \n",
    "    def train(self,file_name,vocab_size):\n",
    "        spm.SentencePieceTrainer.train(input=file_name,\n",
    "                                       model_prefix =self.model_prefix,\n",
    "                                       vocab_size = vocab_size,\n",
    "                                       control_symbols='<start>,<end>,<pad>')\n",
    "        self.sp = spm.SentencePieceProcessor(model_file = self.model_file)\n",
    "    def __call__(self,x,out_type=None):\n",
    "        if type(x) == str:\n",
    "            x = Tokenizer.clean_text(x)   \n",
    "        else:\n",
    "            x = Tokenizer.batched_clean_text(x)\n",
    "        #print(x)\n",
    "        return self.sp.encode(x,out_type)\n",
    "    def detokenize(self,tokens):\n",
    "        return self.sp.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "daa8085c-86b9-4527-8f63-edb4e455cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_shift(tokens):\n",
    "    return tokens[:,1:]\n",
    "def right_shift(tokens):\n",
    "    return tokens[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1052db9e-cc62-4366-8326-0f9c4d947be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padder:\n",
    "    def __init__(self,tokenizer,max_len):\n",
    "        self.sp = tokenizer.sp\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = self.sp.piece_to_id(\"<pad>\")\n",
    "        self.start_token = self.sp.piece_to_id(\"<start>\")\n",
    "        self.end_token = self.sp.piece_to_id(\"<end>\")\n",
    "    def add_pads(self,tokens,max_len=None):\n",
    "        if max_len==None:\n",
    "            max_len = self.max_len\n",
    "        pad_mask = [1]*(len(tokens)+2)\n",
    "        if len(tokens)+2>=max_len:\n",
    "            return [self.start_token]+tokens[:max_len-2]+[self.end_token],pad_mask[:max_len]\n",
    "        pads_ = [self.pad_token]*(max_len-2-len(tokens))\n",
    "        pad_mask[max_len:] = [0]*len(pads_)\n",
    "        return [self.start_token]+tokens+[self.end_token]+pads_,pad_mask\n",
    "    @classmethod\n",
    "    def left_shift(cls,tokens,pad_token):\n",
    "        return jnp.concat([tokens[:,1:],jnp.expand_dims(jnp.repeat(jnp.array([pad_token]),tokens.shape[0]),axis=-1)],axis=-1)\n",
    "    @classmethod\n",
    "    def left_shift_mask(cls,padding):\n",
    "        return jnp.concat([padding[:,1:],jnp.expand_dims(jnp.repeat(jnp.array([0]),padding.shape[0]),axis=-1)],axis=-1)\n",
    "    def __call__(self,tokens):\n",
    "        if type(tokens[0])==int:\n",
    "            return self.add_pads(tokens)\n",
    "        else:\n",
    "            pad_map = list(map(self.add_pads,tokens))\n",
    "            return {\"token_ids\":[sentence for sentence,_ in pad_map],\n",
    "                    \"padding_mask\":[pad_mask for _,pad_mask in pad_map]}\n",
    "        \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35ced2fd-64d2-46d3-b809-8adbdda42366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a38a57c-09d2-482e-8112-203675f8aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding = Padder(tokenizer,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "25bd1f95-cd19-4d48-8eae-c2c0349aea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?spm.SentencePieceTrainer.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "26184dee-f958-49bd-ba97-84a1912cf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trans(jnp.array(padding(tokenizer(sentences))['token_ids']),jnp.array(padding(tokenizer(sentences))['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f2b2faba-fe03-4315-81f5-885137823811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "303dbac8-9ef3-4843-ac60-314c242f6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def n_grams(sentence):\n",
    "#     sentences = []\n",
    "#     words = [word for word in sentence.split(\" \")]\n",
    "#     for i in range(2,len(words)+1):\n",
    "#         # sentences.append(\" \".join(words[:i]))\n",
    "#     return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "91018ea1-398e-4dee-8b8c-a533927c5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = n_grams(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b3fd7f12-9c87-44bd-aea6-ce93e6cb7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle d download devicharith/language-translation-englishfrench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c8da830b-c849-43ad-b572-68c061a324a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip language-translation-englishfrench.zip -d \"language_translation_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1b7dfb34-0ab1-4c39-ab26-4613f055dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1b556827-6059-44ac-9606-6af183f7be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"language_translation_data/eng_-french.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1233db60-fd11-44fc-929e-51cae96c595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9eea9445-78a9-43ae-bf95-0d2185d9410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = data[\"English words/sentences\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c2e952e4-97a6-4dbe-bf29-5259c3fe7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.nn.softmax(jnp.array([-jnp.inf,-jnp.inf,-jnp.inf,-jnp.inf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b3644d57-4556-48ea-9ba0-5eb487df04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2fb9e80f-39d1-41d7-ba78-242e602cb5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"en_text.txt\",\"w\") as fp:\n",
    "#     for text in en_text:\n",
    "#         print(text,file=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7b8afe08-d856-4d2d-b944-c7b5e38f39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_text = data[\"French words/sentences\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a5dcacb3-ce12-497e-9844-f6ec45eaed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"fr_text.txt\",\"w\") as fp:\n",
    "#     for text in fr_text:\n",
    "#         print(text,file=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1eecbc4e-b45a-4666-bfb6-e8230799aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4e269515-8b5b-4537-8bb5-61a1f546b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = Tokenizer(\"en_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0b1394e0-9e5f-482e-8d84-51783ba4c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_tokenizer.train(\"en_text.txt\",vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2b984248-c9c8-43f5-88f8-f98c0e350523",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(\"fr_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3fc1592a-fdf4-4665-9474-72e1068af428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fr_tokenizer.train(\"fr_text.txt\",vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3f616993-dc1d-452c-b403-282638d78d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_en = Padder(en_tokenizer,max_len=64)\n",
    "padding_fr = Padder(fr_tokenizer,max_len=64)\n",
    "en_tokens = en_tokenizer(en_text)\n",
    "fr_tokens = fr_tokenizer(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1643d61e-14e9-4aa4-812b-a92f94a23c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text_en = padding_en(en_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "975347ce-8b48-416a-b8db-5909a6ee7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text_fr = padding_fr(fr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9e14e9a9-b946-4690-8045-039392c9dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_text_en['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77b194ae-6796-4eb4-a6f0-d433160da281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_text_en['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96fb9bea-7fbe-4c07-b0e4-1da4e1ae3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embs = Emb(en_tokens[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "edce9b07-e059-4c1f-a7d1-08ae742c783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ea38dc3-e457-441d-bb91-4a793c06a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9e-3\n",
    "step = 4000\n",
    "warmup_steps = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "71fb451b-83cf-4548-a07d-f1caa6aac395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006987712429686843"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(512**-0.5)*min(step**-0.5,step*warmup_steps**-1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "40be2c82-7cca-4144-af7a-b379f5316e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schedular:\n",
    "    def __init__(self,optimizer):\n",
    "        self.optimizer = optimizer\n",
    "    def update(self,d_model,step,warmup_steps):\n",
    "        self.optimizer.lr = (d_model**-0.5)*min(step**-0.5,step*warmup_steps**-1.5)\n",
    "        return self.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "99f493c0-c69a-4053-91a8-5d9dd8800d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "999e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b9456-e914-4b2f-8373-04f4299c1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 \n",
      "Loss:\n",
      "6.8926473\n",
      "Epoch  1 \n",
      "Loss:\n",
      "6.8774495\n",
      "Epoch  2 \n",
      "Loss:\n",
      "6.8621492\n",
      "Epoch  3 \n",
      "Loss:\n",
      "6.8467507\n",
      "Epoch  4 \n",
      "Loss:\n",
      "6.8311764\n",
      "Epoch  5 \n",
      "Loss:\n",
      "6.8154383\n",
      "Epoch  6 \n",
      "Loss:\n",
      "6.7997185\n",
      "Epoch  7 \n",
      "Loss:\n",
      "6.7837515\n",
      "Epoch  8 \n",
      "Loss:\n",
      "6.7678725\n",
      "Epoch  9 \n",
      "Loss:\n",
      "6.7518153\n",
      "Epoch  10 \n",
      "Loss:\n",
      "6.7358557\n",
      "Epoch  11 \n",
      "Loss:\n",
      "6.7200174\n",
      "Epoch  12 \n",
      "Loss:\n",
      "6.7041483\n",
      "Epoch  13 \n",
      "Loss:\n",
      "6.6878127\n",
      "Epoch  14 \n",
      "Loss:\n",
      "6.6718594\n",
      "Epoch  15 \n",
      "Loss:\n",
      "6.6560326\n",
      "Epoch  16 \n",
      "Loss:\n",
      "6.6401478\n",
      "Epoch  17 \n",
      "Loss:\n",
      "6.6241474\n",
      "Epoch  18 \n",
      "Loss:\n",
      "6.6085376\n",
      "Epoch  19 \n",
      "Loss:\n",
      "6.5932787\n",
      "Epoch  20 \n",
      "Loss:\n",
      "6.5780363\n",
      "Epoch  21 \n",
      "Loss:\n",
      "6.5614834\n",
      "Epoch  22 \n",
      "Loss:\n",
      "6.5458536\n",
      "Epoch  23 \n",
      "Loss:\n",
      "6.5306344\n",
      "Epoch  24 \n",
      "Loss:\n",
      "6.5144577\n",
      "Epoch  25 \n",
      "Loss:\n",
      "6.5001183\n",
      "Epoch  26 \n",
      "Loss:\n",
      "6.4854458\n",
      "Epoch  27 \n",
      "Loss:\n",
      "6.4694576\n",
      "Epoch  28 \n",
      "Loss:\n",
      "6.4542513\n",
      "Epoch  29 \n",
      "Loss:\n",
      "6.4409345\n",
      "Epoch  30 \n",
      "Loss:\n",
      "6.4236565\n",
      "Epoch  31 \n",
      "Loss:\n",
      "6.4095564\n",
      "Epoch  32 \n",
      "Loss:\n",
      "6.3970894\n",
      "Epoch  33 \n",
      "Loss:\n",
      "6.3811674\n",
      "Epoch  34 \n",
      "Loss:\n",
      "6.3659296\n",
      "Epoch  35 \n",
      "Loss:\n",
      "6.3526234\n",
      "Epoch  36 \n",
      "Loss:\n",
      "6.3392717\n",
      "Epoch  37 \n",
      "Loss:\n",
      "6.3229265\n",
      "Epoch  38 \n",
      "Loss:\n",
      "6.3088613\n",
      "Epoch  39 \n",
      "Loss:\n",
      "6.2956605\n",
      "Epoch  40 \n",
      "Loss:\n",
      "6.2817375\n",
      "Epoch  41 \n",
      "Loss:\n",
      "6.2682385\n",
      "Epoch  42 \n",
      "Loss:\n",
      "6.2576057\n",
      "Epoch  43 \n",
      "Loss:\n",
      "6.2399936\n",
      "Epoch  44 \n",
      "Loss:\n",
      "6.2278867\n",
      "Epoch  45 \n",
      "Loss:\n",
      "6.2133093\n",
      "Epoch  46 \n",
      "Loss:\n",
      "6.2002953\n",
      "Epoch  47 \n",
      "Loss:\n",
      "6.1852494\n",
      "Epoch  48 \n",
      "Loss:\n",
      "6.1745243\n",
      "Epoch  49 \n",
      "Loss:\n",
      "6.1622114\n",
      "Epoch  50 \n",
      "Loss:\n",
      "6.1493187\n",
      "Epoch  51 \n",
      "Loss:\n",
      "6.1351457\n",
      "Epoch  52 \n",
      "Loss:\n",
      "6.1195677\n",
      "Epoch  53 \n",
      "Loss:\n",
      "6.1072273\n",
      "Epoch  54 \n",
      "Loss:\n",
      "6.0973725\n",
      "Epoch  55 \n",
      "Loss:\n",
      "6.0826087\n",
      "Epoch  56 \n",
      "Loss:\n",
      "6.0719805\n",
      "Epoch  57 \n",
      "Loss:\n",
      "6.0597906\n",
      "Epoch  58 \n",
      "Loss:\n",
      "6.0462734\n",
      "Epoch  59 \n",
      "Loss:\n",
      "6.0343614\n",
      "Epoch  60 \n",
      "Loss:\n",
      "6.0216856\n",
      "Epoch  61 \n",
      "Loss:\n",
      "6.0093446\n",
      "Epoch  62 \n",
      "Loss:\n",
      "5.8882685\r"
     ]
    }
   ],
   "source": [
    "Trans = Transformer(512,2048,8,1,1000)\n",
    "en_tokens,en_mask = padded_text_en['token_ids'][:1024],padded_text_en['padding_mask'][:1024]\n",
    "fr_tokens,fr_mask = padded_text_fr['token_ids'][:1024],padded_text_fr['padding_mask'][:1024]\n",
    "# #Trans.params = params\n",
    "# Trans({'token_ids':en_tokens,'padding_mask':en_mask},{'token_ids':fr_tokens,'padding_mask':fr_mask})\n",
    "#opt = AdamW(lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, lambda_=0.002)\n",
    "opt = AdamW(1e-2,9e-1,999e-3,1e-9,1e-2)\n",
    "trainer = Trainer(Trans,CategoricalCrossEntropy,opt)\n",
    "trainer.train(({'token_ids':en_tokens,'padding_mask':en_mask},\n",
    "                {'token_ids':fr_tokens,'padding_mask':fr_mask}),\n",
    "               fr_tokens,500,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbbd91-ef30-48c9-98d3-caaf8da6dd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bf91123f-3c87-4a40-a8eb-fdcf5438ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = grad(CategoricalCrossEntropy)(Trans,({'token_ids':en_tokens[:32],'padding_mask':en_mask[:32]},\n",
    "                 {'token_ids':fr_tokens[:32],'padding_mask':fr_mask[:32]}),\n",
    "                one_hot(Padder.left_shift(jnp.array(fr_tokens[:32]),5),1000)).params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4b0bf1c5-e821-4327-a70e-99dcc9416493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Out_Embedding'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trans.params.components[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "0af31a1a-9670-4598-8747-6eac3f148be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In_Embeddings [[[ 9.3224767e-04  1.0073498e+00  3.6200152e-03 ...  1.0101473e+00\n",
      "    1.2534949e-03  9.9859720e-01]\n",
      "  [ 8.4026641e-01  5.4348910e-01  8.1956679e-01 ...  9.9350989e-01\n",
      "    5.5423761e-03  9.9502671e-01]\n",
      "  [ 9.1250330e-01 -4.1299888e-01  9.3438274e-01 ...  1.0023692e+00\n",
      "   -2.5457989e-03  9.9514318e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[-8.5690310e-03  1.0059215e+00 -1.4283760e-04 ...  9.9509794e-01\n",
      "    4.2114947e-03  1.0094552e+00]\n",
      "  [ 8.4139228e-01  5.4529727e-01  8.1881285e-01 ...  1.0031143e+00\n",
      "    1.9937446e-03  9.9990916e-01]\n",
      "  [ 9.1022974e-01 -4.0879697e-01  9.4003475e-01 ...  1.0101473e+00\n",
      "    1.4608215e-03  9.9859720e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]]\n",
      "Out_Embeddings [[[ 8.0166347e-03  9.9224848e-01 -8.1883334e-03 ...  9.9917340e-01\n",
      "    2.7528452e-03  1.0069991e+00]\n",
      "  [ 8.4339458e-01  5.3800648e-01  8.1974208e-01 ...  1.0007778e+00\n",
      "    7.3429863e-03  9.9984026e-01]\n",
      "  [ 9.1060454e-01 -4.1475639e-01  9.4174254e-01 ...  9.9877787e-01\n",
      "   -2.7538487e-04  1.0077515e+00]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[ 8.0166347e-03  9.9224848e-01 -8.1883334e-03 ...  9.9917340e-01\n",
      "    2.7528452e-03  1.0069991e+00]\n",
      "  [ 8.3643264e-01  5.4447132e-01  8.1947005e-01 ...  1.0041153e+00\n",
      "    9.2389192e-03  1.0029525e+00]\n",
      "  [ 9.1320485e-01 -4.1342947e-01  9.3390870e-01 ...  9.9411392e-01\n",
      "    8.3655557e-03  9.9576342e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]]\n",
      "Encoder Output: [[[-0.9984703   1.0139     -0.9931905  ...  1.0195019  -0.9980788\n",
      "    0.99618775]\n",
      "  [ 0.6595812   0.01123097  0.61427474 ...  0.9942685  -1.1639946\n",
      "    0.9973634 ]\n",
      "  [ 0.81525356 -2.0911653   0.8631321  ...  1.0122472  -1.1913604\n",
      "    0.99618864]\n",
      "  ...\n",
      "  [ 0.5305844  -0.26436487 -0.31024587 ...  0.07341229  0.0319322\n",
      "   -1.8928419 ]\n",
      "  [ 0.44177026 -0.02809395 -0.35051757 ... -0.02809395 -1.7119709\n",
      "   -1.8417479 ]\n",
      "  [ 0.0404845  -0.24133608 -0.28497547 ...  0.0404845  -1.65925\n",
      "   -1.7902491 ]]\n",
      "\n",
      " [[-1.0171124   1.011266   -1.0003593  ...  0.9896323  -0.99180955\n",
      "    1.0181205 ]\n",
      "  [ 0.6617465   0.01494216  0.6123379  ...  1.0149226  -1.1718833\n",
      "    1.0077044 ]\n",
      "  [ 0.81014353 -2.083308    0.8754278  ...  1.0292699  -1.1835487\n",
      "    1.0037185 ]\n",
      "  ...\n",
      "  [ 0.5305844  -0.26436487 -0.31024587 ...  0.07341229  0.0319322\n",
      "   -1.8928419 ]\n",
      "  [ 0.44177026 -0.02809395 -0.35051757 ... -0.02809395 -1.7119709\n",
      "   -1.8417479 ]\n",
      "  [ 0.0404845  -0.24133608 -0.28497547 ...  0.0404845  -1.65925\n",
      "   -1.7902491 ]]]\n",
      "Decoder Output: [[[-9.8499137e-01  9.8430580e-01 -1.0175213e+00 ...  9.9778628e-01\n",
      "   -9.9574602e-01  1.0135611e+00]\n",
      "  [ 6.6455019e-01 -1.7167231e-03  6.1282712e-01 ...  1.0077569e+00\n",
      "   -1.1599667e+00  1.0058292e+00]\n",
      "  [ 8.1119275e-01 -2.0946219e+00  8.7935299e-01 ...  1.0042877e+00\n",
      "   -1.1862082e+00  1.0240813e+00]\n",
      "  ...\n",
      "  [ 1.0543153e+00  2.3068705e+00  7.3007755e-02 ... -1.0403755e+00\n",
      "   -9.8700422e-01  2.1670375e+00]\n",
      "  [ 1.0516624e+00  2.3198857e+00  3.4613211e-02 ... -1.0692308e+00\n",
      "   -1.0151919e+00  2.1783035e+00]\n",
      "  [ 1.0709013e+00  2.3402069e+00  5.2984048e-02 ... -1.0518020e+00\n",
      "   -9.9771702e-01  2.1985037e+00]]\n",
      "\n",
      " [[-9.8499137e-01  9.8430651e-01 -1.0175211e+00 ...  9.9778569e-01\n",
      "   -9.9574649e-01  1.0135611e+00]\n",
      "  [ 6.4998984e-01  1.2779207e-02  6.1285132e-01 ...  1.0158117e+00\n",
      "   -1.1558896e+00  1.0133917e+00]\n",
      "  [ 8.1712836e-01 -2.0923414e+00  8.6242461e-01 ...  9.9434805e-01\n",
      "   -1.1676176e+00  9.9808484e-01]\n",
      "  ...\n",
      "  [ 1.0543153e+00  2.3068705e+00  7.3007755e-02 ... -1.0403755e+00\n",
      "   -9.8700422e-01  2.1670375e+00]\n",
      "  [ 1.0516624e+00  2.3198857e+00  3.4613211e-02 ... -1.0692308e+00\n",
      "   -1.0151919e+00  2.1783035e+00]\n",
      "  [ 1.0709013e+00  2.3402069e+00  5.2984048e-02 ... -1.0518020e+00\n",
      "   -9.9771702e-01  2.1985037e+00]]]\n",
      "Linear Output: [[[ 0.01099324 -0.00934422  0.00712648 ... -0.00987655  0.01354216\n",
      "   -0.00121949]\n",
      "  [ 0.01625501 -0.01201183  0.01367321 ... -0.0100228   0.00926699\n",
      "   -0.00447269]\n",
      "  [ 0.02010822 -0.00842213  0.01544059 ... -0.00792194  0.00317\n",
      "   -0.00487469]\n",
      "  ...\n",
      "  [-0.02377193  0.02374615  0.02991716 ...  0.00128742 -0.00264304\n",
      "   -0.00512118]\n",
      "  [-0.03376555  0.02337131  0.0281026  ...  0.00930144  0.00260862\n",
      "    0.01947838]\n",
      "  [-0.04366897  0.02928631  0.03308512 ... -0.00634677  0.00374463\n",
      "    0.0050036 ]]\n",
      "\n",
      " [[ 0.01099324 -0.00934422  0.00712649 ... -0.00987655  0.01354215\n",
      "   -0.00121948]\n",
      "  [ 0.01604734 -0.01219461  0.01386882 ... -0.01009848  0.00903775\n",
      "   -0.00428864]\n",
      "  [ 0.020347   -0.0086524   0.01542515 ... -0.00808469  0.00325461\n",
      "   -0.00495301]\n",
      "  ...\n",
      "  [-0.02377193  0.02374615  0.02991716 ...  0.00128742 -0.00264304\n",
      "   -0.00512118]\n",
      "  [-0.03376555  0.02337131  0.0281026  ...  0.00930144  0.00260862\n",
      "    0.01947838]\n",
      "  [-0.04366897  0.02928631  0.03308512 ... -0.00634677  0.00374463\n",
      "    0.0050036 ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[0.00101055, 0.0009902 , 0.00100665, ..., 0.00098968,\n",
       "         0.00101313, 0.00099828],\n",
       "        [0.00101595, 0.00098763, 0.00101333, ..., 0.0009896 ,\n",
       "         0.00100887, 0.00099511],\n",
       "        [0.00101997, 0.00099128, 0.00101522, ..., 0.00099178,\n",
       "         0.00100284, 0.0009948 ],\n",
       "        ...,\n",
       "        [0.00097679, 0.00102432, 0.00103066, ..., 0.00100157,\n",
       "         0.00099764, 0.00099517],\n",
       "        [0.00096693, 0.00102379, 0.00102864, ..., 0.00100948,\n",
       "         0.00100275, 0.00101981],\n",
       "        [0.00095738, 0.00102983, 0.00103375, ..., 0.00099378,\n",
       "         0.00100386, 0.00100513]],\n",
       "\n",
       "       [[0.00101055, 0.0009902 , 0.00100665, ..., 0.00098968,\n",
       "         0.00101313, 0.00099828],\n",
       "        [0.00101573, 0.00098744, 0.00101352, ..., 0.00098951,\n",
       "         0.00100863, 0.00099528],\n",
       "        [0.00102022, 0.00099106, 0.00101522, ..., 0.00099163,\n",
       "         0.00100293, 0.00099474],\n",
       "        ...,\n",
       "        [0.00097679, 0.00102432, 0.00103066, ..., 0.00100157,\n",
       "         0.00099764, 0.00099517],\n",
       "        [0.00096693, 0.00102379, 0.00102864, ..., 0.00100948,\n",
       "         0.00100275, 0.00101981],\n",
       "        [0.00095738, 0.00102983, 0.00103375, ..., 0.00099378,\n",
       "         0.00100386, 0.00100513]]], dtype=float32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trans({'token_ids':padded_text_en['token_ids'][0:2],'padding_mask':padded_text_en['padding_mask'][0:2]},\n",
    "      {'token_ids':padded_text_fr['token_ids'][0:2],'padding_mask':padded_text_fr['padding_mask'][0:2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "091de010-cf3d-45d2-ab3e-5159f0caf6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6.907756, dtype=float32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CategoricalCrossEntropy(Trans,({'token_ids':padded_text_en['token_ids'][65:68],'padding_mask':padded_text_en['padding_mask'][65:68]},\n",
    "      {'token_ids':padded_text_fr['token_ids'][65:68],'padding_mask':padded_text_fr['padding_mask'][65:68]}),\n",
    "              one_hot(Padder.left_shift(jnp.array(padded_text_fr['token_ids'][65:68]),5),1000))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b062d8f6-9382-4f6e-bd19-407c4133747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 9.3224780e-06  1.0000736e+00  3.6200152e-05 ...  1.0001014e+00\n",
      "    1.2534951e-05  9.9998599e-01]\n",
      "  [ 8.4145898e-01  5.4033417e-01  8.2183337e-01 ...  9.9993509e-01\n",
      "    1.5805045e-04  9.9995029e-01]\n",
      "  [ 9.0932953e-01 -4.1611534e-01  9.3639439e-01 ...  1.0000237e+00\n",
      "    1.7979540e-04  9.9995142e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[-8.5690306e-05  1.0000592e+00 -1.4283750e-06 ...  9.9995100e-01\n",
      "    4.2114949e-05  1.0000945e+00]\n",
      "  [ 8.4147024e-01  5.4035223e-01  8.2182580e-01 ...  1.0000311e+00\n",
      "    1.2256415e-04  9.9999911e-01]\n",
      "  [ 9.0930676e-01 -4.1607332e-01  9.3645090e-01 ...  1.0001014e+00\n",
      "    2.1986160e-04  9.9998599e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[-8.5690306e-05  1.0000592e+00 -1.4283750e-06 ...  9.9995100e-01\n",
      "    4.2114949e-05  1.0000945e+00]\n",
      "  [ 8.4147024e-01  5.4035223e-01  8.2182580e-01 ...  1.0000311e+00\n",
      "    1.2256415e-04  9.9999911e-01]\n",
      "  [ 9.0930676e-01 -4.1607332e-01  9.3645090e-01 ...  1.0001014e+00\n",
      "    2.1986160e-04  9.9998599e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 9.3224780e-06  1.0000736e+00  3.6200152e-05 ...  1.0001014e+00\n",
      "    1.2534951e-05  9.9998599e-01]\n",
      "  [ 8.4156358e-01  5.4030287e-01  8.2186604e-01 ...  9.9996108e-01\n",
      "    1.9859619e-04  9.9999744e-01]\n",
      "  [ 9.0931535e-01 -4.1622040e-01  9.3636978e-01 ...  9.9989927e-01\n",
      "    2.1626531e-04  1.0000604e+00]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[ 2.9208783e-05  1.0000321e+00  2.5703210e-05 ...  1.0000355e+00\n",
      "   -6.8579116e-05  9.9996024e-01]\n",
      "  [ 8.4148031e-01  5.4037577e-01  8.2189244e-01 ...  1.0001014e+00\n",
      "    1.1619827e-04  9.9998599e-01]\n",
      "  [ 9.0920919e-01 -4.1616479e-01  9.3633914e-01 ...  9.9998182e-01\n",
      "    2.2462942e-04  9.9996173e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[ 2.9208783e-05  1.0000321e+00  2.5703210e-05 ...  1.0000355e+00\n",
      "   -6.8579116e-05  9.9996024e-01]\n",
      "  [ 8.4148031e-01  5.4037577e-01  8.2189244e-01 ...  1.0001014e+00\n",
      "    1.1619827e-04  9.9998599e-01]\n",
      "  [ 9.0920919e-01 -4.1616479e-01  9.3633914e-01 ...  9.9998182e-01\n",
      "    2.2462942e-04  9.9996173e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]]\n",
      "[[[ 8.0166348e-05  9.9992251e-01 -8.1883336e-05 ...  9.9999171e-01\n",
      "    2.7528455e-05  1.0000700e+00]\n",
      "  [ 8.4149027e-01  5.4027933e-01  8.2183510e-01 ...  1.0000077e+00\n",
      "    1.7605655e-04  9.9999839e-01]\n",
      "  [ 9.0931052e-01 -4.1613290e-01  9.3646801e-01 ...  9.9998778e-01\n",
      "    2.0249953e-04  1.0000775e+00]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[ 8.0166348e-05  9.9992251e-01 -8.1883336e-05 ...  9.9999171e-01\n",
      "    2.7528455e-05  1.0000700e+00]\n",
      "  [ 8.4142065e-01  5.4034394e-01  8.2183242e-01 ...  1.0000411e+00\n",
      "    1.9501589e-04  1.0000296e+00]\n",
      "  [ 9.0933657e-01 -4.1611964e-01  9.3638968e-01 ...  9.9994111e-01\n",
      "    2.8890895e-04  9.9995762e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[ 8.0166348e-05  9.9992251e-01 -8.1883336e-05 ...  9.9999171e-01\n",
      "    2.7528455e-05  1.0000700e+00]\n",
      "  [ 8.4142065e-01  5.4034394e-01  8.2183242e-01 ...  1.0000411e+00\n",
      "    1.9501589e-04  1.0000296e+00]\n",
      "  [ 9.0933657e-01 -4.1611964e-01  9.3638968e-01 ...  9.9994111e-01\n",
      "    2.8890895e-04  9.9995762e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 8.0166348e-05  9.9992251e-01 -8.1883336e-05 ...  9.9999171e-01\n",
      "    2.7528455e-05  1.0000700e+00]\n",
      "  [ 8.4149027e-01  5.4027933e-01  8.2183510e-01 ...  1.0000077e+00\n",
      "    1.7605655e-04  9.9999839e-01]\n",
      "  [ 9.0931243e-01 -4.1611001e-01  9.3648523e-01 ...  9.9993312e-01\n",
      "    1.2040498e-04  1.0000548e+00]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[ 8.0166348e-05  9.9992251e-01 -8.1883336e-05 ...  9.9999171e-01\n",
      "    2.7528455e-05  1.0000700e+00]\n",
      "  [ 8.4151173e-01  5.4022610e-01  8.2186514e-01 ...  9.9995083e-01\n",
      "    4.2968924e-05  9.9999458e-01]\n",
      "  [ 9.0930223e-01 -4.1614401e-01  9.3649244e-01 ...  9.9992275e-01\n",
      "    1.6010676e-04  9.9996990e-01]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]\n",
      "\n",
      " [[ 8.0166348e-05  9.9992251e-01 -8.1883336e-05 ...  9.9999171e-01\n",
      "    2.7528455e-05  1.0000700e+00]\n",
      "  [ 8.4149027e-01  5.4027933e-01  8.2183510e-01 ...  1.0000077e+00\n",
      "    1.7605655e-04  9.9999839e-01]\n",
      "  [ 9.0933418e-01 -4.1616908e-01  9.3637007e-01 ...  9.9992573e-01\n",
      "    2.9354013e-04  1.0000379e+00]\n",
      "  ...\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]\n",
      "  [ 1.0000000e-12  1.0000000e-12  1.0000000e-12 ...  1.0000000e-12\n",
      "    1.0000000e-12  1.0000000e-12]]]\n",
      "[[[-0.99954623  0.99980706 -1.00043    ...  1.0002435  -1.0020812\n",
      "    0.9978213 ]\n",
      "  [ 0.66151637  0.00319848  0.61772907 ...  1.0070034  -1.1777952\n",
      "    1.0048437 ]\n",
      "  [ 0.808985   -2.0995593   0.8679857  ...  1.0079812  -1.1878039\n",
      "    1.0056782 ]\n",
      "  ...\n",
      "  [ 0.4802024  -0.31406137 -0.3622472  ...  0.02386015 -1.801484\n",
      "   -1.9406507 ]\n",
      "  [ 0.48942727 -0.28166023 -0.32844    ...  0.04640087 -1.7256805\n",
      "    0.00610251]\n",
      "  [-0.02919936 -0.32115135 -0.36861217 ...  0.01168567 -1.7861938\n",
      "   -1.9232666 ]]\n",
      "\n",
      " [[-0.9997327   0.9997808  -1.0005016  ...  0.99994504 -1.0020185\n",
      "    0.99804056]\n",
      "  [ 0.6615378   0.00323551  0.61770964 ...  1.0072094  -1.177874\n",
      "    1.0049468 ]\n",
      "  [ 0.80893373 -2.0994806   0.86810887 ...  1.0081513  -1.1877257\n",
      "    1.0057536 ]\n",
      "  ...\n",
      "  [ 0.4802024  -0.31406137 -0.3622472  ...  0.02386015 -1.801484\n",
      "   -1.9406507 ]\n",
      "  [ 0.48942727 -0.28166023 -0.32844    ...  0.04640087 -1.7256805\n",
      "    0.00610251]\n",
      "  [-0.02919936 -0.32115135 -0.36861217 ...  0.01168567 -1.7861938\n",
      "   -1.9232666 ]]\n",
      "\n",
      " [[-0.9997327   0.9997808  -1.0005016  ...  0.99994504 -1.0020185\n",
      "    0.99804056]\n",
      "  [ 0.6615378   0.00323551  0.61770964 ...  1.0072094  -1.177874\n",
      "    1.0049468 ]\n",
      "  [ 0.80893373 -2.0994806   0.86810887 ...  1.0081513  -1.1877257\n",
      "    1.0057536 ]\n",
      "  ...\n",
      "  [ 0.4802024  -0.31406137 -0.3622472  ...  0.02386015 -1.801484\n",
      "   -1.9406507 ]\n",
      "  [ 0.48942727 -0.28166023 -0.32844    ...  0.04640087 -1.7256805\n",
      "    0.00610251]\n",
      "  [-0.02919936 -0.32115135 -0.36861217 ...  0.01168567 -1.7861938\n",
      "   -1.9232666 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.9995462   0.9998071  -1.0004299  ...  1.0002439  -1.0020809\n",
      "    0.99782145]\n",
      "  [ 0.66174257  0.00313133  0.61779845 ...  1.0070564  -1.1776994\n",
      "    1.0049429 ]\n",
      "  [ 0.8089556  -2.0997782   0.8679333  ...  1.0077091  -1.1877154\n",
      "    1.0059186 ]\n",
      "  ...\n",
      "  [ 0.4802024  -0.31406137 -0.3622472  ...  0.02386015 -1.801484\n",
      "   -1.9406507 ]\n",
      "  [ 0.48942727 -0.28166023 -0.32844    ...  0.04640087 -1.7256805\n",
      "    0.00610251]\n",
      "  [-0.02919936 -0.32115135 -0.36861217 ...  0.01168567 -1.7861938\n",
      "   -1.9232666 ]]\n",
      "\n",
      " [[-0.99949914  0.9997372  -1.0004437  ...  1.0001249  -1.0022362\n",
      "    0.99778277]\n",
      "  [ 0.6615426   0.00327471  0.61783814 ...  1.007343   -1.1778913\n",
      "    1.0048981 ]\n",
      "  [ 0.8087367  -2.099685    0.86788106 ...  1.0079072  -1.1877124\n",
      "    1.0057186 ]\n",
      "  ...\n",
      "  [ 0.4802024  -0.31406137 -0.3622472  ...  0.02386015 -1.801484\n",
      "   -1.9406507 ]\n",
      "  [ 0.48942727 -0.28166023 -0.32844    ...  0.04640087 -1.7256805\n",
      "    0.00610251]\n",
      "  [-0.02919936 -0.32115135 -0.36861217 ...  0.01168567 -1.7861938\n",
      "   -1.9232666 ]]\n",
      "\n",
      " [[-0.99949914  0.9997372  -1.0004437  ...  1.0001249  -1.0022362\n",
      "    0.99778277]\n",
      "  [ 0.6615426   0.00327471  0.61783814 ...  1.007343   -1.1778913\n",
      "    1.0048981 ]\n",
      "  [ 0.8087367  -2.099685    0.86788106 ...  1.0079072  -1.1877124\n",
      "    1.0057186 ]\n",
      "  ...\n",
      "  [ 0.4802024  -0.31406137 -0.3622472  ...  0.02386015 -1.801484\n",
      "   -1.9406507 ]\n",
      "  [ 0.48942727 -0.28166023 -0.32844    ...  0.04640087 -1.7256805\n",
      "    0.00610251]\n",
      "  [-0.02919936 -0.32115135 -0.36861217 ...  0.01168567 -1.7861938\n",
      "   -1.9232666 ]]]\n",
      "[[[-0.99867105  1.0024457  -1.0001178  ...  0.998773   -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.6622639   0.0060745   0.61823    ...  1.0059536  -1.1768433\n",
      "    1.0095885 ]\n",
      "  [ 0.81018585 -2.0967295   0.8686559  ...  1.0080085  -1.1869417\n",
      "    1.0082046 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.99867105  1.0024458  -1.0001178  ...  0.9987729  -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.6621182   0.00621948  0.6182302  ...  1.0060339  -1.1768028\n",
      "    1.0096643 ]\n",
      "  [ 0.8102453  -2.0967066   0.8684866  ...  1.0079088  -1.1867559\n",
      "    1.0079443 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.99867105  1.0024458  -1.0001178  ...  0.9987729  -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.6621182   0.00621948  0.6182302  ...  1.0060339  -1.1768028\n",
      "    1.0096643 ]\n",
      "  [ 0.8102453  -2.0967066   0.8684866  ...  1.0079088  -1.1867559\n",
      "    1.0079443 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.9986712   1.0024457  -1.0001177  ...  0.9987732  -1.0011071\n",
      "    1.0025848 ]\n",
      "  [ 0.6622638   0.00607445  0.6182299  ...  1.0059536  -1.1768433\n",
      "    1.0095885 ]\n",
      "  [ 0.8101907  -2.0966704   0.86869425 ...  1.0078887  -1.1871156\n",
      "    1.0081547 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.99867123  1.0024459  -1.0001177  ...  0.998773   -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.66231525  0.00596549  0.6183005  ...  1.0058327  -1.1771232\n",
      "    1.0095837 ]\n",
      "  [ 0.81016785 -2.0967667   0.86871034 ...  1.007867   -1.1870438\n",
      "    1.0079695 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.998671    1.0024459  -1.0001177  ...  0.998773   -1.0011073\n",
      "    1.0025848 ]\n",
      "  [ 0.66226393  0.00607469  0.61823004 ...  1.0059536  -1.1768435\n",
      "    1.0095886 ]\n",
      "  [ 0.8102402  -2.0968337   0.868444   ...  1.0078764  -1.1867586\n",
      "    1.008122  ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]]\n",
      "[[[-0.99867105  1.0024457  -1.0001178  ...  0.998773   -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.6622639   0.0060745   0.61823    ...  1.0059536  -1.1768433\n",
      "    1.0095885 ]\n",
      "  [ 0.81018585 -2.0967295   0.8686559  ...  1.0080085  -1.1869417\n",
      "    1.0082046 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.99867105  1.0024458  -1.0001178  ...  0.9987729  -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.6621182   0.00621948  0.6182302  ...  1.0060339  -1.1768028\n",
      "    1.0096643 ]\n",
      "  [ 0.8102453  -2.0967066   0.8684866  ...  1.0079088  -1.1867559\n",
      "    1.0079443 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.99867105  1.0024458  -1.0001178  ...  0.9987729  -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.6621182   0.00621948  0.6182302  ...  1.0060339  -1.1768028\n",
      "    1.0096643 ]\n",
      "  [ 0.8102453  -2.0967066   0.8684866  ...  1.0079088  -1.1867559\n",
      "    1.0079443 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.9986712   1.0024457  -1.0001177  ...  0.9987732  -1.0011071\n",
      "    1.0025848 ]\n",
      "  [ 0.6622638   0.00607445  0.6182299  ...  1.0059536  -1.1768433\n",
      "    1.0095885 ]\n",
      "  [ 0.8101907  -2.0966704   0.86869425 ...  1.0078887  -1.1871156\n",
      "    1.0081547 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.99867123  1.0024459  -1.0001177  ...  0.998773   -1.0011072\n",
      "    1.0025848 ]\n",
      "  [ 0.66231525  0.00596549  0.6183005  ...  1.0058327  -1.1771232\n",
      "    1.0095837 ]\n",
      "  [ 0.81016785 -2.0967667   0.86871034 ...  1.007867   -1.1870438\n",
      "    1.0079695 ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]\n",
      "\n",
      " [[-0.998671    1.0024459  -1.0001177  ...  0.998773   -1.0011073\n",
      "    1.0025848 ]\n",
      "  [ 0.66226393  0.00607469  0.61823004 ...  1.0059536  -1.1768435\n",
      "    1.0095886 ]\n",
      "  [ 0.8102402  -2.0968337   0.868444   ...  1.0078764  -1.1867586\n",
      "    1.008122  ]\n",
      "  ...\n",
      "  [ 1.1145848   0.07406896  0.09800313 ... -1.0561646  -1.0024452\n",
      "    2.268838  ]\n",
      "  [ 1.0794917   2.3521655   0.05792508 ... -1.0517255  -0.99898434\n",
      "    0.05792508]\n",
      "  [ 1.0602678   2.3267596   0.04366334 ... -1.0605972   0.04366334\n",
      "    2.187996  ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['om celuiTT voisardue chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisardue chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisardue chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTTueueue chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisueue chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisardue chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisardardimporte première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisardard chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisardardimporte première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé',\n",
       " 'om celuiTT voisardue chaussures première fou cha fou jusqu fatigué chaussures retour me fatigué me fou chaussures me me fou me fou me me fou fatigué fatigué chaussures demandé fatigué fou fou me chaussures me fer cha fatigué cassé fou fou cassé casséelle fou fatigué fou fatigué fou me demandé mees me fou cassé cassé Qui cassé']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fr_tokenizer.detokenize(tokens.tolist()) for tokens in jnp.argmax(Trans({'token_ids':padded_text_en['token_ids'][0:10],'padding_mask':padded_text_en['padding_mask'][0:10]},\n",
    "      {'token_ids':padded_text_fr['token_ids'][0:10],'padding_mask':padded_text_fr['padding_mask'][0:10]}),axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "5cce818c-3790-40ce-8861-a526aacb38b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Salut !',\n",
       " 'Cours !',\n",
       " 'Courez !',\n",
       " 'Qui ?',\n",
       " 'Ça alors !',\n",
       " 'Au feu !',\n",
       " \"À l'aide !\",\n",
       " 'Saute .',\n",
       " 'Ça suffit !',\n",
       " 'Stop !']"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_tokenizer.detokenize(padded_text_fr['token_ids'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "16fbea88-6b1b-4c04-a9e1-17ba0af83df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 11, 214, 205,  88,   4,  88,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 165,   9,  88,   4,  88,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 165,   9,  88,   4,  88,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [156,  17,   4,  88,  79,  79,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [156,  88,  88,   4,  88,  79,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,  45,  88,  88,   4,  88,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 381,  25,   8,  35,  88,   4,  88,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 214, 118,   7,   6,   4,  88,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,   7,  66,  61,  61,  88,   4,  88,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,  86,  82,  88,   4,  88,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,   7,  12, 186,  88,   4,  88,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 318,  16, 188,  82,  88,   4,  88,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 318,  16, 188,  82,  88,   4,  88,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,  13,   9,  22,  45,   7,   6,   4,  88,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,   8,  73,  39,  45,   7,   6,   4,  88,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [  7,  13,   9,  22,  45, 110,   7,   6,   4,  88,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 381,  61,  22,   4,  88,   4,  88,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11, 214, 205,  88,   4,  88,  79,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,  62,   9,   7,   6,   4,  88,  79,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79],\n",
       "       [ 11,   8,  22,  14,   7,   6,   4,  88,  79,  79,  79,  79,  79,\n",
       "         79,  79,  79]], dtype=int32)"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.argmax(Trans({'token_ids':padded_text_en['token_ids'][0:20],'padding_mask':padded_text_en['padding_mask'][0:20]},\n",
    "      {'token_ids':padded_text_fr['token_ids'][0:20],'padding_mask':padded_text_fr['padding_mask'][0:20]}),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "79b1e40f-6fab-4a05-96a4-6908902cdee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Who?',\n",
       " 'Wow!',\n",
       " 'Fire!',\n",
       " 'Help!',\n",
       " 'Jump.',\n",
       " 'Stop!',\n",
       " 'Stop!']"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "4b5e4c55-9a1e-46e1-92f3-a99394f7529d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Salut!',\n",
       " 'Cours\\u202f!',\n",
       " 'Courez\\u202f!',\n",
       " 'Qui ?',\n",
       " 'Ça alors\\u202f!',\n",
       " 'Au feu !',\n",
       " \"À l'aide\\u202f!\",\n",
       " 'Saute.',\n",
       " 'Ça suffit\\u202f!',\n",
       " 'Stop\\u202f!']"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "df96e02b-cfbc-4547-b039-8e7af3a92f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fous le camp !',\n",
       " \"Pars d'ici .\",\n",
       " \"Va t'en !\",\n",
       " 'Disparais !',\n",
       " 'Allez -vous en !',\n",
       " 'Rentrez à la maison .',\n",
       " 'Rentre à la maison .',\n",
       " 'Rentre chez toi .',\n",
       " 'Rentrez chez vous .',\n",
       " 'Va doucement !']"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_tokenizer.detokenize(padded_text_fr['token_ids'][120:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "5b26d1a2-5489-4779-b43e-3c91f65cedd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call us .', 'Come in .', 'Come in .', 'Come in .']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.detokenize(padded_text_en['token_ids'][96:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3399669d-440e-4d4a-ad12-ce5a9aec5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_text_en['token_ids'][120:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "901b4547-8431-4cf5-9b5d-b6cadb9821d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 0.00373945,  0.98045063,  0.01434524, ...,  1.0075562 ,\n",
       "          0.02784876,  1.0029453 ],\n",
       "        [ 0.8286708 ,  0.5332071 ,  0.8206814 , ...,  0.9911458 ,\n",
       "          0.01865378,  1.0128202 ],\n",
       "        [ 0.90645593, -0.3918722 ,  0.947119  , ...,  0.99548614,\n",
       "         -0.02322226,  1.0044475 ],\n",
       "        ...,\n",
       "        [-0.65509343, -0.7416719 ,  0.3009365 , ...,  1.0073984 ,\n",
       "          0.00354829,  1.0133736 ],\n",
       "        [-0.9794912 ,  0.16063708, -0.6112334 , ...,  1.007398  ,\n",
       "          0.00365195,  1.0133733 ],\n",
       "        [-0.3954972 ,  0.921128  , -0.9920058 , ...,  1.0073977 ,\n",
       "          0.00375561,  1.0133729 ]],\n",
       "\n",
       "       [[ 0.00373945,  0.98045063,  0.01434524, ...,  1.0075562 ,\n",
       "          0.02784876,  1.0029453 ],\n",
       "        [ 0.8428125 ,  0.5345792 ,  0.8314176 , ...,  0.9894027 ,\n",
       "          0.00636642,  1.0055251 ],\n",
       "        [ 0.8900799 , -0.42708257,  0.9419847 , ...,  0.99656487,\n",
       "         -0.0027082 ,  1.0100876 ],\n",
       "        ...,\n",
       "        [-0.65509343, -0.7416719 ,  0.3009365 , ...,  1.0073984 ,\n",
       "          0.00354829,  1.0133736 ],\n",
       "        [-0.9794912 ,  0.16063708, -0.6112334 , ...,  1.007398  ,\n",
       "          0.00365195,  1.0133733 ],\n",
       "        [-0.3954972 ,  0.921128  , -0.9920058 , ...,  1.0073977 ,\n",
       "          0.00375561,  1.0133729 ]]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emb(jnp.array(padded_text_en['token_ids'][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "02499a3e-ec66-454c-b5e5-ed17934338c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self,model,en_tokenizer,fr_tokenizer,en_padder,fr_padder):\n",
    "        self.model = model\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_padder = en_padder\n",
    "        self.fr_padder = fr_padder\n",
    "    def __call__(self,text):\n",
    "        en_tokens = self.en_tokenizer([text])\n",
    "        #print(en_tokens)\n",
    "        padded_text_en = self.en_padder(en_tokens)\n",
    "        print(padded_text_en)\n",
    "        #fr_tokens = [[3]]\n",
    "        count = 0\n",
    "        pred_token = 0\n",
    "        fr_text = \"A\"\n",
    "        while pred_token!=4:\n",
    "            fr_tokens = self.fr_tokenizer([fr_text])\n",
    "            padded_text_fr = self.fr_padder(fr_tokens)\n",
    "            #print(padded_text_fr['token_ids'])\n",
    "            index=padded_text_fr['token_ids'][0].index(4)\n",
    "            padded_text_fr['token_ids'][0][index] = 5\n",
    "            padded_text_fr['padding_mask'][0][index] = 0\n",
    "            print(padded_text_fr)\n",
    "            predicted_tokens = jnp.argmax(self.model({'token_ids':padded_text_en['token_ids'],'padding_mask':padded_text_en['padding_mask']},\n",
    "                   {'token_ids':padded_text_fr['token_ids'],'padding_mask':padded_text_fr['padding_mask']}),axis=-1)\n",
    "            #print(predicted_tokens)\n",
    "            pred_token = predicted_tokens[0][count]\n",
    "            fr_text+=self.fr_tokenizer.detokenize(pred_token.tolist())\n",
    "            count+=1\n",
    "        #print(predicted_tokens)\n",
    "        print([self.fr_tokenizer.detokenize(tokens.tolist()) for tokens in predicted_tokens])\n",
    "        print(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "6f6ae808-5caa-436e-b236-a60ed907be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Later = Translator(Trans,en_tokenizer,fr_tokenizer,padding_en,padding_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "35d3ddbd-0ada-4207-85cc-a0ac0e81cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': [[3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]], 'padding_mask': [[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'token_ids': [[3, 213, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]], 'padding_mask': [[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'token_ids': [[3, 213, 88, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]], 'padding_mask': [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'token_ids': [[3, 213, 88, 88, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]], 'padding_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "['S !rararararararararararara']\n",
      "A!!\n"
     ]
    }
   ],
   "source": [
    "Later(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ba01bffe-8a61-4f1d-ac3a-60760da0b335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saute.'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_text[7]R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0447cadb-2549-4b2b-9048-686c3cae9840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.', 'Run!', 'Run!', 'Who?']"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_text[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "4f6ae1ff-9df2-4def-ad6f-3a8d2a082c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Salut!', 'Cours\\u202f!', 'Courez\\u202f!', 'Qui ?']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_text[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef8fc3a7-80b1-4beb-9245-512db27f19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxlib\n",
    "def convert_weights(params):\n",
    "    params = copy.deepcopy(params)\n",
    "    for key in params:\n",
    "        if type(params[key])==jaxlib.xla_extension.ArrayImpl:\n",
    "            params[key]=params[key].tolist()\n",
    "        else:\n",
    "            params[key] = convert_weights(params[key])\n",
    "    return params\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4789e779-d252-4934-92de-a84618e5d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = convert_weights(Trans.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90b37759-7910-46b5-9ced-6d00fa30517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_string = json.dumps(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b6186df-981d-48ad-bdc7-b83c9205fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights-6-512.json\",\"w\") as file:\n",
    "    file.write(weight_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "d4b0efa5-82aa-4f66-848e-fa96012a19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weights_jax(params):\n",
    "    params = copy.deepcopy(params)\n",
    "    for key in params:\n",
    "        if type(params[key])==list:\n",
    "            params[key]=jnp.array(params[key])\n",
    "        else:\n",
    "            params[key] = convert_weights_jax(params[key])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "9056f1af-d627-4f6e-94c1-b0976a4949a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights.json\",\"r\") as file:\n",
    "    weight_string = file.read()\n",
    "    params = json.loads(weight_string)\n",
    "    params = convert_weights_jax(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "f988db7e-5b41-48eb-a95b-a9b9ffcebfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7dba4c54-a2fc-4e37-9d96-7170cce744bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clip_gradients(params):\n",
    "#     params = copy.deepcopy(params)\n",
    "#     for key in params:\n",
    "#         if type(params[key])==jaxlib.xla_extension.ArrayImpl:\n",
    "#             params[key] = jnp.clip(params[key],-1.0,1.0)\n",
    "#             params[key] = jnp.nan_to_num(params[key])\n",
    "#         else:\n",
    "#             params[key] = clip_gradients(params[key])\n",
    "#     return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4218358-f49a-4798-ba61-3af20d21cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa7094-b73f-4cf4-86cf-71d3018cfe4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
