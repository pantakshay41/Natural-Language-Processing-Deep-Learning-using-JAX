{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f3a98fd-e195-4e24-a1ea-4f537a90622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "import jax\n",
    "from jax import grad,vmap\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.tree_util import register_pytree_node_class\n",
    "import numpy as np\n",
    "from jax import lax as jlax\n",
    "from jax.tree_util import register_pytree_node_class\n",
    "import json\n",
    "#import copyself.components\n",
    "import jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "771ee6e4-ef9c-4f43-a667-45d8a26d1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class Parameter:\n",
    "    \"\"\"\n",
    "    A class to represent a parameter with a name and a value, supporting basic arithmetic operations\n",
    "    and integration with the JAX pytree system.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the parameter.\n",
    "    value : ndarray\n",
    "        The value of the parameter, typically a NumPy array or similar.\n",
    "    shape : tuple\n",
    "        The shape of the value.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,value):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        self.shape = value.shape\n",
    "    def __sub__(self,param):\n",
    "        if isinstance(param,Parameter):\n",
    "            return Parameter(self.name,self.value-param.value)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(param)} and 'Parameter'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,Parameter):\n",
    "            return Parameter(self.name,self.value+other.value)\n",
    "        if isinstance(other,float):\n",
    "            return Parameter(self.name,self.value+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'Parameter'\")\n",
    "    def __mul__(self,factor):\n",
    "        if isinstance(factor,float):\n",
    "            return Parameter(self.name,factor*self.value)\n",
    "        raise TypeError(f'Cannot multiply a Parameter with {type(factor)}')\n",
    "    def __rmul__(self,factor):\n",
    "        if isinstance(factor,float):\n",
    "            return Parameter(self.name,factor*self.value)\n",
    "        raise TypeError(f'Cannot multiply a Parameter with {type(factor)}')\n",
    "    def __pow__(self,factor):\n",
    "        return Parameter(self.name,self.value**factor)\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,Parameter):\n",
    "            return Parameter(self.name,self.value/other.value)\n",
    "        if isinstance(other,float):\n",
    "            return Parameter(self.name,self.value/other)\n",
    "        raise TypeError(f'Cannot divide a Parameter with {type(other)}')\n",
    "    def tree_flatten(self):\n",
    "        children = (self.value,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ae3dc55-f13a-4fa3-add9-e59aa6565d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class LinearParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a linear layer, specifically the weights, which are stored\n",
    "    as a Parameter object.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the linear layer parameters.\n",
    "    weights : Parameter\n",
    "        The weights of the linear layer, stored as a Parameter object.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,weights):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"W\",weights)\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,LinearParams):\n",
    "            return LinearParams(self.name,self.weights-other.weights)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'LinearParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,LinearParams) :\n",
    "            return LinearParams(self.name,self.weights+other.weights)\n",
    "        if isinstance(other,float) :\n",
    "            return LinearParams(self.name,self.weights+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'LinearParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'LinearParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'LinearParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,LinearParams) :\n",
    "            return LinearParams(self.name,self.weights/other.weights)\n",
    "        if isinstance(other,float):\n",
    "            return LinearParams(self.name,self.weights/other)\n",
    "        raise TypeError(f\"Cannot divide a 'LinearParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return LinearParams(self.name,self.weights**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a02a9f5-9ff6-47ba-972f-dbb93a51396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class FeedForwardParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a feedforward neural network layer, specifically the weights and biases,\n",
    "    which are stored as Parameter objects.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the feedforward layer parameters.\n",
    "    weights : Parameter\n",
    "        The weights of the feedforward layer, stored as a Parameter object.\n",
    "    bias : Parameter\n",
    "        The bias of the feedforward layer, stored as a Parameter object.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,weights,bias):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"W\",weights)\n",
    "        if isinstance(bias,Parameter):\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = Parameter(\"bais\",bias)\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,FeedForwardParams) :\n",
    "            return FeedForwardParams(self.name,self.weights-other.weights,self.bias-other.bias)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'FeedForwardParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,FeedForwardParams):\n",
    "            return FeedForwardParams(self.name,self.weights+other.weights,self.bias+other.bias)\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights+other,self.bias+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'FeedForwardParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights*other,self.bias*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights*other,self.bias*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,FeedForwardParams):\n",
    "            return FeedForwardParams(self.name,self.weights/other.weights,self.bias/other.bias)\n",
    "        if isinstance(other,float):\n",
    "            return FeedForwardParams(self.name,self.weights/other,self.bias/other)\n",
    "        raise TypeError(f\"Cannot divide a 'FeedForwardParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return FeedForwardParams(self.name,self.weights**factor,self.bias**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,self.bias,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79eceadc-72b0-4905-8719-c53652142344",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class AttentionParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of an attention mechanism, specifically the weights for key, query, and value,\n",
    "    which are stored as Parameter objects.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the attention parameters.\n",
    "    w_k : Parameter\n",
    "        The weights for the key, stored as a Parameter object.\n",
    "    w_q : Parameter\n",
    "        The weights for the query, stored as a Parameter object.\n",
    "    w_v : Parameter\n",
    "        The weights for the value, stored as a Parameter object.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,w_k,w_q,w_v):\n",
    "        self.name = name\n",
    "        if isinstance(w_k,Parameter):\n",
    "            self.w_k = w_k\n",
    "        else:\n",
    "            self.w_k = Parameter(\"w_k\",w_k)\n",
    "        if isinstance(w_q,Parameter):\n",
    "            self.w_q = w_q\n",
    "        else:\n",
    "            self.w_q = Parameter(\"w_q\",w_q)\n",
    "        if isinstance(w_v,Parameter):\n",
    "            self.w_v = w_v\n",
    "        else:\n",
    "            self.w_v = Parameter(\"w_v\",w_v)\n",
    "        \n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k-other.w_k,self.w_q-other.w_q,self.w_v-other.w_v)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'AttentionParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k+other.w_k,self.w_q+other.w_q,self.w_v+other.w_v)\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k+other,self.w_q+other,self.w_v+other)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'AttentionParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k*other,self.w_q*other,self.w_v*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'AttentionParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,AttentionParams):\n",
    "            return AttentionParams(self.name,self.w_k/other.w_k,self.w_q/other.w_q,self.w_v/other.w_v)\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k/other,self.w_q/other,self.w_v/other)\n",
    "        raise TypeError(f\"Cannot divide a 'AttentionParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            return AttentionParams(self.name,self.w_k*other,self.w_q*other,self.w_v*other)\n",
    "        raise TypeError(f\"Cannot multiply a 'AttentionParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        return AttentionParams(self.name,self.w_k**factor,self.w_q**factor,self.w_v**factor)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.w_k,self.w_q,self.w_v,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "711444b0-84c7-4af1-a136-3e49310b2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class MultiHeadAttentionParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a multi-head attention mechanism, including the output weights\n",
    "    and a list of individual attention heads.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the multi-head attention parameters.\n",
    "    weights : Parameter\n",
    "        The output weights of the multi-head attention, stored as a Parameter object.\n",
    "    heads : list[AttentionParams]\n",
    "        A list of AttentionParams objects representing the individual attention heads.\n",
    "    num_heads : int\n",
    "        The number of attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,weights,heads:list[AttentionParams]):\n",
    "        self.name = name\n",
    "        if isinstance(weights,Parameter):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = Parameter(\"Wo\",weights)\n",
    "        self.heads = heads\n",
    "        self.num_heads = len(heads)\n",
    "    def add(self,head1,head2):\n",
    "        return head1+head2\n",
    "    def subtract(self,head1,head2):\n",
    "        return head1-head2\n",
    "    def multiply(self,val,head):\n",
    "        return val*head\n",
    "    def divide(self,head,val):\n",
    "        return head/val\n",
    "    def pow(self,val,head):\n",
    "        return head**val\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams):\n",
    "            heads = list(map(self.subtract,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights-other.weights,heads)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'MultiHeadAttentionParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams) :\n",
    "            heads = list(map(self.add,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights+other.weights,heads)\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.add,self.heads,[other]*self.num_heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights+other,heads)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'MultiHeadAttentionParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.multiply,[other]*self.num_heads,self.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights*other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,MultiHeadAttentionParams) :\n",
    "            heads = list(map(self.divide,self.heads,other.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights/other.weights,heads)\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.divide,self.heads,[other]*self.num_heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights/other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            heads = list(map(self.multiply,[other]*self.num_heads,self.heads))\n",
    "            return MultiHeadAttentionParams(self.name,self.weights*other,heads)\n",
    "        raise TypeError(f\"Cannot multiply a 'MultiHeadAttentionParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        heads = list(map(self.pow,[factor]*self.num_heads,self.heads))\n",
    "        return MultiHeadAttentionParams(self.name,self.weights**factor,heads)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.weights,self.heads,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5101d899-f9b7-4f86-b980-972ce088b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class ModuleParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters of a module, including a list of its components.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the module parameters.\n",
    "    components : list\n",
    "        A list of components that make up the module.\n",
    "    num_comps : int\n",
    "        The number of components in the module.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,components):\n",
    "        self.name = name\n",
    "        self.components = components\n",
    "        self.num_comps = len(components)\n",
    "    def multiply(self,val,comp):\n",
    "        return val*comp\n",
    "    def subtract(self,comp1,comp2):\n",
    "        return comp1-comp2\n",
    "    def add(self,comp1,comp2):\n",
    "        return comp1+comp2\n",
    "    def pow(self,val,comp):\n",
    "        return comp**val\n",
    "    def divide(self,comp,val):\n",
    "        return comp/val\n",
    "    def __sub__(self,other):\n",
    "        if isinstance(other,ModuleParams):\n",
    "            comps = list(map(self.subtract,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"unsupported operand type(s) for -: {type(other)} and 'ModuleParams'\")\n",
    "    def __add__(self,other):\n",
    "        if isinstance(other,ModuleParams) :\n",
    "            comps = list(map(self.add,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.add,self.components,[other]*self.num_comps))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"unsupported operand type(s) for +: {type(other)} and 'ModuleParams'\")\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.multiply,[other]*self.num_comps,self.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot multiply a 'ModuleParams' with {type(other)}\")\n",
    "    def __truediv__(self,other):\n",
    "        if isinstance(other,ModuleParams) :\n",
    "            comps = list(map(self.divide,self.components,other.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.divide,self.components,[other]*self.num_comps))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot divide a 'ModuleParams' with {type(other)}\")\n",
    "    def __rmul__(self,other):\n",
    "        if isinstance(other,float):\n",
    "            comps = list(map(self.multiply,[other]*self.num_comps,self.components))\n",
    "            return ModuleParams(self.name,comps)\n",
    "        raise TypeError(f\"Cannot multiply a 'ModuleParams' with {type(other)}\")\n",
    "    def __pow__(self,factor):\n",
    "        comps = list(map(self.pow,[factor]*self.num_comps,self.components))\n",
    "        return ModuleParams(self.name,comps)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.components,)\n",
    "        aux_data = (self.name,)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee510652-2fcb-4349-a7e4-591b8ff7e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    A class to represent dropout regularization in a neural network.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    dropout_p : float\n",
    "        The probability of dropping out a unit, defaulting to 0.2.\n",
    "    seed : int\n",
    "        The seed for random number generation, defaulting to 0.\n",
    "    \"\"\"\n",
    "    def __init__(self,dropout_p,seed=0):\n",
    "        self.dropout_p = 0.2\n",
    "        self.seed = seed\n",
    "    def predict(self,x):\n",
    "        import random as rnd\n",
    "        _,key_ = random.split(random.key(rnd.randint(0,1000)))\n",
    "        mask_ = random.bernoulli(key_,1-self.dropout_p,shape=x.shape)\n",
    "        dropout_out = mask_*x\n",
    "        scale = 1/(1-self.dropout_p)\n",
    "        return dropout_out*scale\n",
    "    def batched_predict(self,x):\n",
    "        predictor = vmap(self.predict,in_axes=(0))\n",
    "        return predictor(x)\n",
    "    def __call__(self,x):\n",
    "        if len(x.shape)>1:\n",
    "            return self.batched_predict(x)\n",
    "        return self.predict(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bf926c2-f341-447e-8e5c-774a2a4f592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class LinearLayer:\n",
    "    \"\"\"\n",
    "    A class to represent a linear layer in a neural network.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    in_units : int\n",
    "        The number of input units for the linear layer.\n",
    "    out_units : int\n",
    "        The number of output units for the linear layer.\n",
    "    params : LinearParams\n",
    "        The parameters of the linear layer.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "\n",
    "    Class Methods:\n",
    "    --------------\n",
    "    initiate_params(cls, name, in_units, out_units, key, scale=1e-2):\n",
    "        Initializes the parameters for the linear layer.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def initiate_params(cls,name,in_units,out_units,key,scale=1e-2):\n",
    "        w_key,_= random.split(key,2)\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        params = {}\n",
    "        #params[\"W\"] = random.normal(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params[\"W\"] = initializer(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        params = LinearParams(name,initializer(w_key,shape = (in_units,out_units),dtype=jnp.float32)*scale)\n",
    "        return params\n",
    "    def __init__(self,name,in_units,out_units,params=None):\n",
    "        self.in_units = in_units\n",
    "        self.out_units = out_units\n",
    "        self.params = params\n",
    "        self.key = random.key(210)\n",
    "        if params==None:\n",
    "            self.params = LinearLayer.initiate_params(name,self.n_vocab,self.embedding_dims,self.key)\n",
    "    def predict(self,x):\n",
    "        x = jnp.matmul(x,self.params.weights.value)\n",
    "        return x\n",
    "    def batched_predict(self,x):\n",
    "        predictor = vmap(self.predict,in_axes=[0])\n",
    "        return predictor(x)\n",
    "    def __call__(self,x):\n",
    "        if len(x.shape)>2:\n",
    "            return self.batched_predict(x)\n",
    "        return self.predict(x)\n",
    "        #print(x)\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.in_units,self.out_units)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9676c47a-95c6-4a1c-8676-98d822ee432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class EmbeddingLayer:\n",
    "    \"\"\"\n",
    "    A class to represent an embedding layer in a neural network, including parameter initialization \n",
    "    and positional encoding.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    n_vocab : int\n",
    "        The size of the vocabulary for the embedding layer.\n",
    "    embedding_dims : int\n",
    "        The dimensionality of the embeddings.\n",
    "    params : LinearParams\n",
    "        The parameters of the embedding layer.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "\n",
    "    Class Methods:\n",
    "    --------------\n",
    "    initiate_params(cls, name, n_vocab, embedding_dims, key, scale=1e-1):\n",
    "        Initializes the parameters for the embedding layer.\n",
    "    positional_enc(cls, emb_dims, seq_len):\n",
    "        Computes positional encodings for a given sequence length and embedding dimensions.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    one_hot(self, x, max):\n",
    "        Converts input indices into one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def initiate_params(cls,name,n_vocab,embedding_dims,key,scale=1e-1):\n",
    "        w_key,_= random.split(key,2)\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        params = {}\n",
    "        #params[\"W\"] = random.normal(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params[\"W\"] = initializer(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale\n",
    "        params = LinearParams(name,initializer(w_key,shape = (n_vocab,embedding_dims),dtype=jnp.float32)*scale)\n",
    "        return params   \n",
    "    @classmethod\n",
    "    def positional_enc(cls,emb_dims,seq_len):\n",
    "        pos = jnp.arange(seq_len)[:, jnp.newaxis]\n",
    "        pe = jnp.zeros((seq_len,emb_dims))\n",
    "        div_terms = jnp.exp(jnp.arange(0, emb_dims, 2) * -(jnp.log(10000.0) / emb_dims))\n",
    "        pe = pe.at[:, 0::2].set(jnp.sin(pos*div_terms))\n",
    "        pe = pe.at[:, 1::2].set(jnp.cos(pos*div_terms))\n",
    "        return pe\n",
    "    def one_hot(self,x,max):\n",
    "        return jnp.array(x[:,:,None]==jnp.arange(max),dtype=jnp.float32)\n",
    "    def __init__(self,name,n_vocab,embedding_dims,params=None):\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.params = params\n",
    "        self.key = random.key(210)\n",
    "        if params==None:\n",
    "            self.params = EmbeddingLayer.initiate_params(name,self.n_vocab,self.embedding_dims,self.key)\n",
    "    def predict(self,x,mask):\n",
    "        seq_len = x.shape[-1]\n",
    "        x = self.one_hot(x,self.n_vocab)\n",
    "        x = jnp.matmul(x,self.params.weights.value)+EmbeddingLayer.positional_enc(self.embedding_dims,seq_len)\n",
    "        mask = jnp.expand_dims(mask,axis=-1)\n",
    "        x=x*mask+jnp.ones(shape=mask.shape)*1e-12\n",
    "        return x\n",
    "    def batched_predict(self,x,mask):\n",
    "        predictor = vmap(self.predict,in_axes=[0,0])\n",
    "        return predictor(x)\n",
    "    def __call__(self,x,mask):\n",
    "        if len(x.shape)>2:\n",
    "            return self.batched_predict(x,mask)\n",
    "        return self.predict(x,mask)\n",
    "        #print(x)\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.n_vocab,self.embedding_dims)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f1c2cff-6420-4a2d-ace6-abddf9a70ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Applies the Rectified Linear Unit (ReLU) activation function element-wise.\n",
    "\n",
    "    The ReLU function is defined as:\n",
    "        ReLU(x) = max(0, x)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        The input array or tensor to which the ReLU function will be applied.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    jnp.ndarray\n",
    "        An array or tensor of the same shape as `x`, with the ReLU function applied element-wise.\n",
    "    \"\"\"\n",
    "    return jnp.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "738a31eb-590a-43a7-b3c5-0a023ca651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class FeedForward:\n",
    "    \"\"\"\n",
    "    A class to represent a feedforward neural network layer with configurable activation function, weights, and biases.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    activation : callable\n",
    "        The activation function to apply after the linear transformation. Defaults to the identity function.\n",
    "    units : int\n",
    "        The number of units (or neurons) in the feedforward layer.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    d_model : int\n",
    "        The dimensionality of the input to the feedforward layer.\n",
    "    params : FeedForwardParams\n",
    "        The parameters of the feedforward layer, including weights and biases.\n",
    "\n",
    "    Class Methods:\n",
    "    --------------\n",
    "    initiate_params(name, input_shape, units, key, scale=1e-4):\n",
    "        Initializes the parameters for the feedforward layer.\n",
    "    \"\"\"\n",
    "    def initiate_params(name,input_shape,units,key,scale=1e-4):\n",
    "        w_key,b_key = random.split(key,2)\n",
    "        params = {}\n",
    "        #params[\"W\"] = random.normal(w_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        #params[\"b\"] = random.normal(b_key,shape = (units,))*scale\n",
    "        \n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params[\"W\"] = initializer(w_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        params = FeedForwardParams(name,\n",
    "                                   weights = initializer(w_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                   bias = random.normal(b_key,shape = (units,))*scale)\n",
    "        #params[\"b\"] = initializer(b_key,shape = (units,))*scale\n",
    "        return params\n",
    "    def __init__(self,name,d_model,units,activation=lambda x:x,params=None):\n",
    "        self.activation = activation\n",
    "        self.units = units\n",
    "        self.key = random.key(210)\n",
    "        self.d_model = d_model\n",
    "        if params == None:\n",
    "            self.params = FeedForward.initiate_params(name,d_model,self.units,self.key)\n",
    "        else:\n",
    "            self.params = params\n",
    "    def predict(self,input):\n",
    "        return self.activation(jnp.matmul(input,self.params.weights.value)+self.params.bias.value)\n",
    "    def batched_predict(self,inputs):\n",
    "        predictor = vmap(self.predict,in_axes = (0))\n",
    "        return predictor(inputs)\n",
    "    def __call__(self,input):\n",
    "        if len(input.shape)>1:\n",
    "            return self.batched_predict(input)\n",
    "        return self.predict(input)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.units,self.activation)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43ececb6-534d-476e-b0ca-03228abe9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class AttentionHead:\n",
    "    \"\"\"\n",
    "    A class to represent an individual attention head in a multi-head attention mechanism.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    d : int\n",
    "        The dimensionality of the query, key, and value vectors for the attention head.\n",
    "    d_model : int\n",
    "        The dimensionality of the input to the attention head.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    params : AttentionParams\n",
    "        The parameters of the attention head, including weights for query, key, and value.\n",
    "\n",
    "    Class Methods:\n",
    "    --------------\n",
    "    initiate_params(name, input_shape, units, key, scale=1e-2):\n",
    "        Initializes the parameters for the attention head.\n",
    "    \"\"\"\n",
    "    def initiate_params(name,input_shape,units,key,scale=1e-2):\n",
    "        q_key,k_key,v_key = random.split(key,3)\n",
    "        \n",
    "        params = {}\n",
    "        # params[\"Wq\"] = random.normal(q_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params[\"Wk\"] = random.normal(k_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params['Wv'] = random.normal(v_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        # params[\"Wq\"] = initializer(q_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params[\"Wk\"] = initializer(k_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        # params['Wv'] = initializer(v_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        params = AttentionParams(name=name,\n",
    "                                 w_q = initializer(q_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                 w_k = initializer(k_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                 w_v = initializer(v_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "                                )\n",
    "                                 \n",
    "        return params\n",
    "    def __init__(self,name,d,d_model,params=None):\n",
    "        self.d = d\n",
    "        self.d_model = d_model \n",
    "        self.key = random.key(210)\n",
    "        self.params = params\n",
    "        if params ==None:\n",
    "            self.params = AttentionHead.initiate_params(name,self.d_model,self.d,self.key)\n",
    "    def predict(self,x_q,x_k,x_v,mask,decoder=False):\n",
    "        query = jnp.matmul(x_q,self.params.w_q.value)\n",
    "        key = jnp.matmul(x_k,self.params.w_k.value)\n",
    "        value = jnp.matmul(x_v,self.params.w_v.value)\n",
    "        #print(\"Attenion Shapes:\",query.shape,key.shape,value.shape)\n",
    "        attn_scores = jnp.matmul(query,key.T)/jnp.sqrt(self.d)\n",
    "        if mask != None:\n",
    "            mask = jnp.expand_dims(mask,axis=0)\n",
    "            #print(mask*mask.T)\n",
    "            attn_scores = attn_scores*(mask*mask.T) +(mask*mask.T!=1)*(-1e-20)\n",
    "        #print(attn_scores)\n",
    "        #print(attn_scores.shape)\n",
    "        softmaxed_attn = jax.nn.softmax(attn_scores)\n",
    "        if mask != None:\n",
    "            softmaxed_attn = softmaxed_attn*(mask*mask.T) +(mask*mask.T!=1)*(1e-32)\n",
    "        #softmaxed_attn = jnp.nan_to_num(softmaxed_attn)\n",
    "        #print(softmaxed_attn)\n",
    "        if decoder:\n",
    "            softmaxed_attn = softmaxed_attn*jnp.triu(jnp.ones(attn_scores.shape))\n",
    "        #print(softmaxed_attn)\n",
    "        #print(\"Value Matrix:\",value.shape)\n",
    "        return jnp.matmul(softmaxed_attn,value)\n",
    "    def batched_predict(self,x_q,x_k,x_v,mask,decoder=False):\n",
    "        predictor = vmap(self.predict,in_axes = (0,0,0,0,None))\n",
    "        return predictor(x_q,x_k,x_v,mask,decoder)\n",
    "    def __call__(self,x_q,x_k,x_v,mask,decoder=False):\n",
    "        if len(x_q.shape)>1:\n",
    "            #print(x_q.shape)\n",
    "            return self.batched_predict(x_q,x_k,x_v,mask,decoder)\n",
    "        return self.predict(x_q,x_k,x_v,mask,decoder)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d,self.d_model)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddd21ca5-f990-429c-a66a-730a34eeb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    A class to represent a multi-head attention mechanism in a neural network.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    h : int\n",
    "        The number of attention heads.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for the multi-head attention.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    d : int\n",
    "        The dimensionality of each attention head.\n",
    "    params : MultiHeadAttentionParams\n",
    "        The parameters of the multi-head attention, including output weights and individual attention heads.\n",
    "    attentionHeads : list[AttentionHead]\n",
    "        A list of AttentionHead instances representing each attention head.\n",
    "\n",
    "    Class Methods:\n",
    "    --------------\n",
    "    initiate_params(name, num_heads, input_shape, units, key, scale=1e-3):\n",
    "        Initializes the parameters for the multi-head attention mechanism.\n",
    "    \"\"\"\n",
    "    def initiate_params(name,num_heads,input_shape,units,key,scale=1e-3):\n",
    "        o_key,*h_key = random.split(key,num_heads+1)\n",
    "        #print(o_key,h_key)\n",
    "        params = {}\n",
    "        #params['Wo'] = random.normal(o_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        initializer = jax.nn.initializers.he_normal()\n",
    "        #params['Wo'] = initializer(o_key,shape = (input_shape,units),dtype=jnp.float32)*scale\n",
    "        params = MultiHeadAttentionParams(name,\n",
    "                                          weights = initializer(o_key,shape = (input_shape,units),dtype=jnp.float32)*scale,\n",
    "                                          heads = [AttentionHead.initiate_params(f\"H{i}\",\n",
    "                                                                                 input_shape,\n",
    "                                                                                 input_shape//num_heads,\n",
    "                                                                                 h_key[i]) \n",
    "                                                   for i in range(num_heads)\n",
    "                                                  ]\n",
    "                                         )\n",
    "        return params\n",
    "    def __init__(self,name,h,d_model,params=None):\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.key = random.key(210)\n",
    "        self.d = d_model//h\n",
    "        self.params = params\n",
    "        if params ==None:\n",
    "            self.params = MultiHeadAttention.initiate_params(name,self.h,self.d_model,self.d_model,self.key)\n",
    "        \n",
    "        if self.d_model%self.h!=0:\n",
    "            raise \"D_model not divisible by number of heads\"\n",
    "        self.attentionHeads = [AttentionHead(f\"H{i}\",\n",
    "                                             self.d,\n",
    "                                             self.d_model,\n",
    "                                             self.params.heads[i]) \n",
    "                               for i in range(self.h)]\n",
    "        \n",
    "    #def predict(self,x_q,x_k,x_v,mask=None,decoder=False):\n",
    "        #return jnp.matmul(self.params['Wo'],jnp.concat([head.predict(x_q_i,x_k_i,x_v_i,mask,decoder) for head,x_q_i,x_k_i,x_v_i in zip(self.attentionHeads,[x_q]*8,[x_k]*8,[x_v]*8)]))\n",
    "    def calc_attentions(self,x_q,x_k,x_v,mask=None,decoder=False):\n",
    "        concat_attn = jnp.concat([head.batched_predict(x_q_i,x_k_i,x_v_i,mask,decoder) \n",
    "                                  for head,x_q_i,x_k_i,x_v_i in \n",
    "                                  zip(self.attentionHeads,[x_q]*8,[x_k]*8,[x_v]*8)],\n",
    "                                 axis=-1)\n",
    "        return concat_attn\n",
    "    def predict(self,attns):\n",
    "        return jnp.matmul(attns,self.params.weights.value)\n",
    "    def batched_predict(self,attns):\n",
    "        predictor = vmap(self.predict,in_axes = (0))\n",
    "        return predictor(attns)\n",
    "    def __call__(self,x_q,x_k,x_v,mask=None,decoder=False):\n",
    "        attns = self.calc_attentions(x_q,x_k,x_v,mask,decoder)\n",
    "        if len(x_q.shape)>1: \n",
    "            return self.batched_predict(attns)\n",
    "        return self.predict(attns)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.h,self.d_model)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "211c7f53-8ee0-49a9-90a4-64f83578d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class EncoderLayer:\n",
    "    \"\"\"\n",
    "    A class to represent an encoder layer in a transformer model, including multi-head attention, \n",
    "    feedforward layers, and dropout.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for the encoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanism.\n",
    "    params : ModuleParams\n",
    "        The parameters of the encoder layer, including multi-head attention and feedforward layers.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    ff1 : FeedForward\n",
    "        The first feedforward network in the encoder layer.\n",
    "    ff2 : FeedForward\n",
    "        The second feedforward network in the encoder layer.\n",
    "    mha : MultiHeadAttention\n",
    "        The multi-head attention mechanism in the encoder layer.\n",
    "    dropout : Dropout\n",
    "        The dropout regularization applied to the encoder layer.\n",
    "\n",
    "    Class Methods:\n",
    "    --------------\n",
    "    layer_normalization(output, epsilon=1e-9):\n",
    "        Applies layer normalization to the input tensor.\n",
    "    \"\"\"\n",
    "    def layer_normalization(output, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        Applies layer normalization to the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        output : jnp.ndarray\n",
    "            The input tensor to be normalized.\n",
    "        epsilon : float, optional\n",
    "            A small constant added to the variance to avoid division by zero. Defaults to 1e-9.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        jnp.ndarray\n",
    "            The normalized tensor.\n",
    "        \"\"\"\n",
    "        H = output.shape[-1]\n",
    "        mean = jnp.expand_dims(output.mean(axis=-1), axis=-1)\n",
    "        std = jnp.expand_dims(output.std(axis=-1), axis=-1)\n",
    "        output = (output - mean) / (std + epsilon)\n",
    "        return output\n",
    "\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,params=None):\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.params = params\n",
    "        self.key = random.key(210)\n",
    "        if params==None:\n",
    "            ff1_key,ff2_key,mha_key = random.split(self.key,3)\n",
    "            self.params = ModuleParams(name,\n",
    "                                       [MultiHeadAttention.initiate_params('mha',num_heads,d_model,d_model,mha_key),\n",
    "                                        FeedForward.initiate_params('ff1',d_model,d_ff,ff1_key),\n",
    "                                        FeedForward.initiate_params('ff2',d_ff,d_model,ff2_key)])\n",
    "        \n",
    "        self.ff1 = FeedForward(\"ff1\",d_model,d_ff,params=self.params.components[1])\n",
    "        self.ff2 = FeedForward(\"ff2\",d_ff,d_model,params=self.params.components[2])\n",
    "        #self.__name__ = f\"EncoderLayer{num}\"\n",
    "        self.mha = MultiHeadAttention(\"mha\",num_heads,d_model,self.params.components[0])\n",
    "        self.dropout = Dropout(0.2)\n",
    "       \n",
    "        #print(self.params)\n",
    "    def predict(self,input,mask):\n",
    "        attentions = self.mha(input,input,input,mask)\n",
    "        #print(mask)\n",
    "        attentions = self.dropout(attentions)\n",
    "        #print(\"Attentions\")\n",
    "        #print(attentions)\n",
    "        x = EncoderLayer.layer_normalization(input+attentions)\n",
    "        #print(\"x+attentions\")\n",
    "       # print(x)\n",
    "        ff_ = self.ff2(self.ff1(x))\n",
    "        ff_ = self.dropout(ff_)\n",
    "        #print(\"x+ff_\")\n",
    "        #print(x+ff_)\n",
    "        x = EncoderLayer.layer_normalization(x+ff_)\n",
    "        #print(x)\n",
    "        return x\n",
    "    # def batched_predict(self,inputs,mask):\n",
    "    #     predictor = vmap(self.predict,in_axes=(0,0))\n",
    "    #     return predictor(inputs,mask)\n",
    "    def __call__(self,inputs,mask):\n",
    "        # if len(inputs.shape)>1:\n",
    "        #     return self.batched_predict(inputs,mask)\n",
    "        return self.predict(inputs,mask)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7016ac8-6054-4978-b103-477dfa190f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EncoderLayer.layer_normalization(random.normal(random.key(210),shape=(64,128,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2acdb622-838c-46ed-a926-2b059b9703b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayerParams:\n",
    "    \"\"\"\n",
    "    A class to encapsulate the parameters for an encoder layer in a transformer model.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    params : ModuleParams\n",
    "        The parameters of the encoder layer, including multi-head attention and feedforward layers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name associated with the encoder layer parameters.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for the encoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanism.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, d_model, d_ff, num_heads, key):\n",
    "        ff1_key, ff2_key, mha_key = random.split(key, 3)\n",
    "        self.params = ModuleParams(\n",
    "            name,\n",
    "            [\n",
    "                MultiHeadAttention.initiate_params('mha', num_heads, d_model, d_model, mha_key),\n",
    "                FeedForward.initiate_params('ff1', d_model, d_ff, ff1_key),\n",
    "                FeedForward.initiate_params('ff2', d_ff, d_model, ff2_key)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "class EncoderParams:\n",
    "    \"\"\"\n",
    "    A class to encapsulate the parameters for an encoder in a transformer model, consisting of multiple encoder layers.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    params : ModuleParams\n",
    "        The parameters of the encoder, including multiple encoder layers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name associated with the encoder parameters.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for each encoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each encoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanism of each encoder layer.\n",
    "    num_layers : int\n",
    "        The number of encoder layers in the encoder.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, d_model, d_ff, num_heads, num_layers, key):\n",
    "        keys = random.split(key, num_layers)\n",
    "        self.params = ModuleParams(\n",
    "            name,\n",
    "            [\n",
    "                EncoderLayerParams(f\"L{i}\", d_model, d_ff, num_heads, key_).params\n",
    "                for i, key_ in enumerate(keys)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "@register_pytree_node_class\n",
    "class Encoder:\n",
    "    \"\"\"\n",
    "    A class to represent an encoder in a transformer model, consisting of multiple encoder layers.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for each encoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each encoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanism of each encoder layer.\n",
    "    num_layers : int\n",
    "        The number of encoder layers in the encoder.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    params : ModuleParams\n",
    "        The parameters of the encoder, including multiple encoder layers.\n",
    "    layers : list[EncoderLayer]\n",
    "        A list of `EncoderLayer` instances representing each layer in the encoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name associated with the encoder.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for each encoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each encoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanism of each encoder layer.\n",
    "    num_layers : int\n",
    "        The number of encoder layers in the encoder.\n",
    "    params : ModuleParams, optional\n",
    "        The parameters of the encoder. If not provided, they will be initialized.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,num_layers,params=None):\n",
    "        #self.num= generate_number(num_layers)\n",
    "        self.d_model=d_model\n",
    "        self.d_ff=d_ff\n",
    "        self.num_heads=num_heads\n",
    "        self.num_layers=num_layers\n",
    "        self.key = random.key(210)\n",
    "        self.params = params\n",
    "        if params==None:\n",
    "            self.params = ModuleParams(name,\n",
    "                                   [EncoderLayerParams(f\"L{i}\",d_model,d_ff,num_heads,key_).params \n",
    "                                    for i,key_ in enumerate(keys)])\n",
    "        self.layers = [EncoderLayer(f\"L{i}\",d_model,d_ff,num_heads,self.params.components[i]) for i in range(num_layers)]\n",
    "    def __call__(self,input,mask):\n",
    "        x = input\n",
    "        #print(\"Encoder Input Shape:\",x.shape)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return x\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads,self.num_layers)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54a4b4a7-cbf0-403e-b54d-23166499bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayerParams:\n",
    "    \"\"\"\n",
    "    A class to encapsulate the parameters for a decoder layer in a transformer model.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    params : ModuleParams\n",
    "        The parameters of the decoder layer, including multi-head attention mechanisms and feedforward layers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name associated with the decoder layer parameters.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for the decoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within the decoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanisms of the decoder layer.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,key):\n",
    "        self.params = {}\n",
    "        ff1_key,ff2_key,e_mha_key,d_mha_key = random.split(key,4)\n",
    "        self.params = ModuleParams(name,\n",
    "                                [MultiHeadAttention.initiate_params('d_mha',num_heads,d_model,d_model,d_mha_key),\n",
    "                                 MultiHeadAttention.initiate_params('e_mha',num_heads,d_model,d_model,e_mha_key),\n",
    "                                 FeedForward.initiate_params('ff1',d_model,d_ff,ff1_key),\n",
    "                                 FeedForward.initiate_params('ff2',d_ff,d_model,ff2_key)])\n",
    "\n",
    "@register_pytree_node_class        \n",
    "class DecoderLayer:\n",
    "    \"\"\"\n",
    "    A class to represent a decoder layer in a transformer model, including multi-head attention mechanisms,\n",
    "    feedforward layers, and dropout.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for the decoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanisms of the decoder layer.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    params : ModuleParams, optional\n",
    "        The parameters of the decoder layer. If not provided, they will be initialized.\n",
    "    ff1 : FeedForward\n",
    "        The first feedforward network in the decoder layer.\n",
    "    ff2 : FeedForward\n",
    "        The second feedforward network in the decoder layer.\n",
    "    d_mha : MultiHeadAttention\n",
    "        The multi-head attention mechanism for decoder inputs in the decoder layer.\n",
    "    e_mha : MultiHeadAttention\n",
    "        The multi-head attention mechanism for encoder outputs in the decoder layer.\n",
    "    dropout : Dropout\n",
    "        The dropout regularization applied to the decoder layer.\n",
    "\n",
    "    Class Methods:\n",
    "    --------------\n",
    "    layer_normalization(output, epsilon=1e-9):\n",
    "        Applies layer normalization to the input tensor.\n",
    "    \"\"\"\n",
    "    def layer_normalization(output, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        Applies layer normalization to the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        output : jnp.ndarray\n",
    "            The input tensor to be normalized.\n",
    "        epsilon : float, optional\n",
    "            A small constant added to the variance to avoid division by zero. Defaults to 1e-9.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        jnp.ndarray\n",
    "            The normalized tensor.\n",
    "        \"\"\"\n",
    "        H = output.shape[-1]\n",
    "        mean = jnp.expand_dims(output.mean(axis=-1), axis=-1)\n",
    "        std = jnp.expand_dims(output.std(axis=-1), axis=-1)\n",
    "        output = (output - mean) / (std + epsilon)\n",
    "        return output\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,params=None):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.key = random.key(210)\n",
    "        self.params = params\n",
    "        if params ==None:\n",
    "            ff1_key,ff2_key,e_mha_key,d_mha_key = random.split(self.key,4)\n",
    "            self.params = ModuleParams(name,\n",
    "                                    [MultiHeadAttention.initiate_params('d_mha',num_heads,d_model,d_model,d_mha_key),\n",
    "                                     MultiHeadAttention.initiate_params('e_mha',num_heads,d_model,d_model,e_mha_key),\n",
    "                                     FeedForward.initiate_params('ff1',d_model,d_ff,ff1_key),\n",
    "                                     FeedForward.initiate_params('ff2',d_ff,d_model,ff2_key)])\n",
    "        self.ff1 = FeedForward(\"ff1\",d_model,d_ff,params = self.params.components[2])\n",
    "        self.ff2 = FeedForward(\"ff2\",d_ff,d_model,params = self.params.components[3])\n",
    "        self.d_mha = MultiHeadAttention(\"d_mha\",num_heads,d_model,params = self.params.components[0])\n",
    "        self.e_mha = MultiHeadAttention(\"e_mha\",num_heads,d_model,params = self.params.components[1])\n",
    "        self.dropout = Dropout(0.2)\n",
    "    def predict(self,input,encoder_output,mask):\n",
    "        attentions = self.d_mha(input,input,input,mask,decoder=True)\n",
    "        attentions = self.dropout(attentions)\n",
    "        x = DecoderLayer.layer_normalization(input+attentions)\n",
    "        e_attentions = self.e_mha(x,encoder_output,encoder_output,mask)\n",
    "        e_attentions = self.dropout(e_attentions)\n",
    "        x = DecoderLayer.layer_normalization(x+e_attentions)\n",
    "        ff_ = self.ff2(self.ff1(x))\n",
    "        ff_ = self.dropout(ff_)\n",
    "        x = DecoderLayer.layer_normalization(x+ff_)\n",
    "        return x\n",
    "    # def batched_predict(self,inputs,encoder_output,mask):\n",
    "    #     predictor = vmap(self.predict,in_axes=(0,0,0))\n",
    "    #     return predictor(inputs,encoder_output,mask)\n",
    "    def __call__(self,inputs,encoder_output,mask):\n",
    "        # if len(inputs.shape)>1:\n",
    "        #     return self.batched_predict(inputs,encoder_output)\n",
    "        return self.predict(inputs,encoder_output,mask)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be858598-5914-4380-9f5f-76b33173b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderParams:\n",
    "    \"\"\"\n",
    "    A class to encapsulate the parameters for a decoder in a transformer model, consisting of multiple decoder layers.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    params : ModuleParams\n",
    "        The parameters of the decoder, including multiple decoder layers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name associated with the decoder parameters.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for each decoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each decoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanisms of each decoder layer.\n",
    "    num_layers : int\n",
    "        The number of decoder layers in the decoder.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for parameter initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,num_layers,key):\n",
    "        keys = random.split(key,num_layers)\n",
    "        self.params = ModuleParams(name,\n",
    "                                   [DecoderLayerParams(f\"L{i}\",d_model,d_ff,num_heads,key_).params \n",
    "                                    for i,key_ in enumerate(keys)])\n",
    "@register_pytree_node_class\n",
    "class Decoder:\n",
    "    \"\"\"\n",
    "    A class to represent a decoder in a transformer model, consisting of multiple decoder layers.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    params : ModuleParams\n",
    "        The parameters of the decoder, including multiple decoder layers.\n",
    "    keys : list[jax.random.PRNGKey]\n",
    "        The random keys used for parameter initialization of each decoder layer.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for each decoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each decoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanisms of each decoder layer.\n",
    "    num_layers : int\n",
    "        The number of decoder layers in the decoder.\n",
    "    layers : list[DecoderLayer]\n",
    "        A list of `DecoderLayer` instances representing each layer in the decoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name associated with the decoder.\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for each decoder layer.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each decoder layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanisms of each decoder layer.\n",
    "    num_layers : int\n",
    "        The number of decoder layers in the decoder.\n",
    "    params : ModuleParams, optional\n",
    "        The parameters of the decoder. If not provided, they will be initialized.\n",
    "    \"\"\"\n",
    "    def __init__(self,name,d_model,d_ff,num_heads,num_layers,params=None):\n",
    "        self.params = params\n",
    "        self.keys = random.split(random.key(251),num_layers)\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        if params==None:\n",
    "            self.params = ModuleParams(name,\n",
    "                                   [DecoderLayerParams(f\"L{i}\",d_model,d_ff,num_heads,key_).params \n",
    "                                    for i,key_ in enumerate(keys)])\n",
    "        self.layers = [DecoderLayer(f\"L{i}\",d_model,d_ff,num_heads,self.params.components[i]) for i in range(num_layers)]\n",
    "    def __call__(self,input,encoder_output,mask):\n",
    "        x = input\n",
    "        #print(mask)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output,mask)\n",
    "        return x\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads,self.num_layers)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "178f068b-18b9-4363-a81c-c02a9bdfc2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mLinearLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_2923/1082525596.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?LinearLayer.initiate_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1da60a8-2c54-4285-abd2-1e46a1dd179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerParams:\n",
    "    \"\"\"\n",
    "    A class to represent the parameters for a transformer model, including embeddings, encoder, decoder, and output linear layer.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    params : dict\n",
    "        A dictionary containing the parameters of the transformer model. This includes:\n",
    "        - Embedding parameters for the input and output tokens\n",
    "        - Encoder parameters\n",
    "        - Decoder parameters\n",
    "        - Linear layer parameters for the output projection\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for the transformer model.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each transformer layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanisms of the encoder and decoder.\n",
    "    num_layers : int\n",
    "        The number of layers in both the encoder and decoder.\n",
    "    n_vocab : int\n",
    "        The size of the vocabulary for token embeddings.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key for initializing the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,d_ff,num_heads,num_layers,n_vocab,key):\n",
    "        self.params = {}\n",
    "        key_emb,key_emb_dec,key_e,key_d,key_l = random.split(key,5)\n",
    "        #self.params[\"Embedding\"] = EmbeddingLayer.initiate_params(n_vocab,d_model,key_emb)\n",
    "        self.params   = ModuleParams(\"Transformer\",\n",
    "                                     [EmbeddingLayer.initiate_params(\"In_Embedding\",n_vocab,d_model,key_emb),\n",
    "                                     EmbeddingLayer.initiate_params(\"Out_Embedding\",n_vocab,d_model,key_emb_dec),\n",
    "                                     EncoderParams(\"Encoder\",d_model,d_ff,num_heads,num_layers,key_e).params,\n",
    "                                     DecoderParams(\"Decoder\",d_model,d_ff,num_heads,num_layers,key_d).params,\n",
    "                                     LinearLayer.initiate_params(\"Linear\",d_model,n_vocab,key_l)])\n",
    "        \n",
    "\n",
    "@register_pytree_node_class\n",
    "class Transformer:\n",
    "    \"\"\"\n",
    "    A class to represent a transformer model, including embeddings, encoder, decoder, and output linear layer.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    d_model : int\n",
    "        The dimensionality of the input and output for the transformer model.\n",
    "    d_ff : int\n",
    "        The dimensionality of the hidden layer in the feedforward network within each transformer layer.\n",
    "    num_heads : int\n",
    "        The number of attention heads in the multi-head attention mechanisms of the encoder and decoder.\n",
    "    num_layers : int\n",
    "        The number of layers in both the encoder and decoder.\n",
    "    n_vocab : int\n",
    "        The size of the vocabulary for token embeddings.\n",
    "    logits : bool\n",
    "        Whether to output logits (if True) or probabilities (if False).\n",
    "    params : dict, optional\n",
    "        The parameters of the transformer model. If not provided, they will be initialized.\n",
    "    key : jax.random.PRNGKey\n",
    "        The random key used for initializing the parameters.\n",
    "    in_embedding : EmbeddingLayer\n",
    "        The embedding layer for input tokens.\n",
    "    out_embedding : EmbeddingLayer\n",
    "        The embedding layer for output tokens.\n",
    "    encoder : Encoder\n",
    "        The encoder component of the transformer model.\n",
    "    decoder : Decoder\n",
    "        The decoder component of the transformer model.\n",
    "    linear : LinearLayer\n",
    "        The linear layer used for output projection.\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,d_ff,num_heads,num_layers,n_vocab,logits=False,params=None,seed=0):\n",
    "        self.d_model = d_model\n",
    "        self.d_ff=d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.n_vocab = n_vocab\n",
    "        self.logits = logits\n",
    "        self.params = params\n",
    "        self.key = random.key(seed)\n",
    "        if params == None:\n",
    "            self.params = TransformerParams(d_model,d_ff,num_heads,num_layers,n_vocab,self.key).params\n",
    "        #self.embedding = EmbeddingLayer(n_vocab,d_model,self.params[\"Embedding\"])\n",
    "        self.in_embedding = EmbeddingLayer(\"In_Embedding\",n_vocab,d_model,self.params.components[0])\n",
    "        self.out_embedding = EmbeddingLayer(\"Out_Embedding\",n_vocab,d_model,self.params.components[1])\n",
    "        self.encoder = Encoder(\"Encoder\",d_model,d_ff,num_heads,num_layers,self.params.components[2])\n",
    "        self.decoder = Decoder(\"Decoder\",d_model,d_ff,num_heads,num_layers,self.params.components[3])\n",
    "        self.linear = LinearLayer(\"Linear\",d_model,n_vocab,params=self.params.components[4])\n",
    "        self.logits = logits\n",
    "    def update_params(self):\n",
    "        self.in_embedding = EmbeddingLayer(\"In_Embedding\",self.n_vocab,self.d_model,self.params.components[0])\n",
    "        self.out_embedding = EmbeddingLayer(\"Out_Embedding\",self.n_vocab,self.d_model,self.params.components[1])\n",
    "        self.encoder = Encoder(\"Encoder\",self.d_model,self.d_ff,self.num_heads,self.num_layers,self.params.components[2])\n",
    "        self.decoder = Decoder(\"Decoder\",self.d_model,self.d_ff,self.num_heads,self.num_layers,self.params.components[3])\n",
    "        self.linear = LinearLayer(\"Linear\",self.d_model,self.n_vocab,params=self.params.components[4])\n",
    "    def __call__(self,inputs,outputs):\n",
    "        input_tokens = jnp.array(inputs['token_ids'])\n",
    "        input_mask = jnp.array(inputs['padding_mask'])\n",
    "        output_tokens = jnp.array(outputs['token_ids'])\n",
    "        output_mask = jnp.array(outputs['padding_mask'])\n",
    "        input_tokens = Padder.left_shift(input_tokens,5)\n",
    "        input_mask = Padder.left_shift_mask(input_mask)\n",
    "        embs = self.in_embedding(input_tokens,input_mask)\n",
    "        #print(\"In_Embeddings\",embs)\n",
    "        op_embs = self.out_embedding(output_tokens,output_mask)\n",
    "        #print(\"Out_Embeddings\",op_embs)\n",
    "        if len(embs.shape)!=3:\n",
    "            raise \"Dimensions of the input must include (Batch,Token Sequence)\"\n",
    "        encoder_output = self.encoder(embs,input_mask)\n",
    "        #print(\"Encoder Output:\",encoder_output)\n",
    "        decoder_output = self.decoder(op_embs,encoder_output,output_mask)\n",
    "        #print(\"Decoder Output:\",decoder_output)\n",
    "        output = self.linear(decoder_output)\n",
    "        #print(\"Linear Output:\",output)\n",
    "        if self.logits:\n",
    "            return output\n",
    "        return jax.nn.softmax(output,axis=-1)\n",
    "    def tree_flatten(self):\n",
    "        children = (self.params,)\n",
    "        aux_data = (self.d_model,self.d_ff,self.num_heads,self.num_layers,self.n_vocab,self.logits)\n",
    "        return (children, aux_data)\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(*aux_data,*children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "faaf70e4-1308-443c-8d0d-90e7deadfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Optimizer:\n",
    "#     def __init__(self,lr,lambda_):\n",
    "#         self.lr = lr\n",
    "#         self.lambda_ = lambda_\n",
    "#     def update_coder_params(self,t,params,grads):\n",
    "#         for layer in params:\n",
    "#             params[layer] = self.update_layer_params(t,\n",
    "#                 params[layer],\n",
    "#                 grads[layer]\n",
    "#             )\n",
    "#         return params\n",
    "#     def update_layer_params(self,t,params,grads):\n",
    "#         for type in params:\n",
    "#             if 'ff' in type:\n",
    "#                 params[type] = self.update_basic_params(t,\n",
    "#                     params[type],\n",
    "#                     grads[type]\n",
    "#                 )\n",
    "#             if 'mha' in type:\n",
    "#                 params[type] = self.update_mha_params(t,\n",
    "#                     params[type],\n",
    "#                     grads[type]\n",
    "#                 )\n",
    "#         return params\n",
    "#     def update_mha_params(self,t,params,grads):\n",
    "#         params['Wo'] = self.update_params(t,params['Wo'],grads['Wo'])\n",
    "#         for head in params:\n",
    "#             if head == 'Wo':\n",
    "#                 continue\n",
    "#             for type in params[head]:\n",
    "#                 #print(head,type)\n",
    "#                 params[head][type] = self.update_params(t,params[head][type],grads[head][type])\n",
    "#         return params\n",
    "#     def update_basic_params(self,t,params,grads):\n",
    "#         for type in params:\n",
    "#             params[type]= self.update_params(t,params[type],grads[type])\n",
    "#         return params\n",
    "#     def update_params(self,t,params,grads):\n",
    "#         params = params - self.lr*(grads)\n",
    "#         return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d48c2b33-810d-4123-97a4-9391b184e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self,lr,lambda_):\n",
    "        self.lr = lr\n",
    "        self.lambda_ = lambda_\n",
    "    def update_params(self,t,params,grads):\n",
    "        params = params - self.lr*grads\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b80582-1f67-4750-9e2c-c82ec4ed5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    \"\"\"\n",
    "    Implementation of the AdamW optimizer using JAX.\n",
    "\n",
    "    AdamW is an optimization algorithm that combines the benefits of Adam (adaptive moment estimation) with weight decay regularization.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    beta1 : float\n",
    "        The exponential decay rate for the first moment estimates.\n",
    "    beta2 : float\n",
    "        The exponential decay rate for the second moment estimates.\n",
    "    epsilon : float\n",
    "        A small constant to prevent division by zero in the update step.\n",
    "    m : array-like or None\n",
    "        The first moment vector, initialized to None until the first update.\n",
    "    v : array-like or None\n",
    "        The second moment vector, initialized to None until the first update.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lr : float\n",
    "        The learning rate for the optimizer.\n",
    "    beta1 : float\n",
    "        The exponential decay rate for the first moment estimates.\n",
    "    beta2 : float\n",
    "        The exponential decay rate for the second moment estimates.\n",
    "    epsilon : float\n",
    "        A small constant to prevent division by zero.\n",
    "    lambda_ : float\n",
    "        The weight decay factor for regularization.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    SetScheduleMultiplier(t):\n",
    "        Returns a multiplier for the learning rate schedule at timestep `t`.\n",
    "    update_params(t, params, grads):\n",
    "        Updates the model parameters using the AdamW optimization algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self,lr,beta1,beta2,epsilon,lambda_):\n",
    "        super().__init__(lr,lambda_)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "    def SetScheduleMultiplier(self,t):\n",
    "        return 0.0001\n",
    "    def update_params(self,t,params,grads):\n",
    "        g = grads + self.lambda_*params\n",
    "        if self.m is None:\n",
    "            self.m = params*0\n",
    "            self.v = params*0\n",
    "\n",
    "        self.m = self.beta1*self.m + (1-self.beta1)*g\n",
    "        self.v = self.beta2*self.v + (1-self.beta2)*(g**2)\n",
    "        m_hat = self.m/(1-(self.beta1)**t)\n",
    "        v_hat = self.v/(1-(self.beta2)**t)\n",
    "        eta = self.SetScheduleMultiplier(t)\n",
    "        params = params - eta*((self.lr*m_hat)/((v_hat)**0.5+self.epsilon) + self.lambda_*params)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88495ac4-79f4-4dcd-85db-39bfaa778a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for managing the training process of a model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be trained, which should have an `update_params` method\n",
    "               to update its parameters and a `params` attribute.\n",
    "        loss: A function that computes the loss given the model, input data, and target values.\n",
    "        optimizer: An optimizer instance with an `update_params` method to update the model's\n",
    "                   parameters based on gradients.\n",
    "        scheduler: (Optional) A scheduler instance or function to adjust the learning rate or\n",
    "                   other hyperparameters during training.\n",
    "\n",
    "    Methods:\n",
    "        update(t, grads):\n",
    "            Updates the model parameters using the optimizer and the provided gradients.\n",
    "        \n",
    "        train(x, y, epochs, batch_size=None):\n",
    "            Trains the model for a specified number of epochs.\n",
    "            \n",
    "            Args:\n",
    "                x: A tuple containing the input data for the model. The first element is a dictionary\n",
    "                   with 'token_ids' and 'padding_mask' for the source input, and the second element\n",
    "                   is a dictionary for the target input with similar keys.\n",
    "                y: The target values corresponding to the input data.\n",
    "                epochs: The number of epochs to train the model.\n",
    "                batch_size: (Optional) The size of each batch. If not provided, the entire dataset is used\n",
    "                            as a single batch.\n",
    "            \n",
    "            Returns:\n",
    "                None\n",
    "    \"\"\"\n",
    "    def __init__(self,model,loss,optimizer,schedular=None):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        #self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.schedular = None\n",
    "        if schedular!=None:\n",
    "            self.schedular = schedular(optimizer)\n",
    "    # def update(self,t,grads):\n",
    "    #     self.model.params[\"Encoder\"] = self.optimizer.update_coder_params(t,self.model.params[\"Encoder\"],grads[\"Encoder\"])\n",
    "    #     self.model.params[\"Decoder\"] = self.optimizer.update_coder_params(t,self.model.params[\"Decoder\"],grads[\"Decoder\"])\n",
    "    #     self.model.params[\"Linear\"] = self.optimizer.update_basic_params(t,self.model.params[\"Linear\"],grads[\"Linear\"])\n",
    "    #     #self.model.params[\"Embedding\"] = self.optimizer.update_basic_params(t,self.model.params[\"Embedding\"],grads[\"Embedding\"])\n",
    "    #     self.model.params[\"In_Embedding\"] = self.optimizer.update_basic_params(t,self.model.params[\"In_Embedding\"],grads[\"In_Embedding\"])\n",
    "    #     self.model.params[\"Out_Embedding\"] = self.optimizer.update_basic_params(t,self.model.params[\"Out_Embedding\"],grads[\"Out_Embedding\"])\n",
    "    def update(self,t,grads):\n",
    "        self.model.params = self.optimizer.update_params(t,self.model.params,grads)\n",
    "        self.model.update_params()\n",
    "    def train(self,x,y,epochs,batch_size=None):\n",
    "        data_size = len(x[0]['token_ids'])\n",
    "        if batch_size== None:\n",
    "            batch_size = data_size\n",
    "        # if learning_rate!=None:\n",
    "        #     self.lr = learning_rate\n",
    "        t = 0\n",
    "        for epoch in range(epochs):\n",
    "            curr = 0\n",
    "            print(\"Epoch \",epoch,\"\\r\",flush=True)\n",
    "            print(\"Loss:\")\n",
    "            while curr<data_size:\n",
    "                t+=1\n",
    "                batch_x = (\n",
    "                    {'token_ids':x[0]['token_ids'][curr:curr+batch_size],\n",
    "                     'padding_mask':x[0]['padding_mask'][curr:curr+batch_size]},\n",
    "                    {'token_ids':x[1]['token_ids'][curr:curr+batch_size],\n",
    "                     'padding_mask':x[1]['padding_mask'][curr:curr+batch_size]}\n",
    "                          )\n",
    "                batch_y = y[curr:curr+batch_size]\n",
    "                #print(Padder.left_shift(jnp.array(batch_y),5))\n",
    "                #break\n",
    "                batch_y = one_hot(Padder.left_shift(jnp.array(batch_y),5),self.model.n_vocab)\n",
    "                curr = curr+batch_size\n",
    "                grads = grad(self.loss)(self.model,batch_x,batch_y).params\n",
    "                #grads = clip_gradients(grads)\n",
    "                self.update(t,grads)\n",
    "                if self.schedular!=None:\n",
    "                    self.schedular.update(512,t,4000)\n",
    "                    self.optimizer = self.schedular.optimizer\n",
    "                    #print(\"Learning Rate:\",self.optimizer.lr)\n",
    "                print(self.loss(self.model,batch_x,batch_y),end=\"\\r\",flush=True)\n",
    "            print(self.loss(self.model,batch_x,batch_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62eda2a5-1f9f-449d-a033-74a41332ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalCrossEntropy(transformer, x, y):\n",
    "    \"\"\"\n",
    "    Computes the categorical cross-entropy loss for a transformer model.\n",
    "\n",
    "    Args:\n",
    "        transformer: A callable model that takes input tokens and output tokens\n",
    "                     and produces predicted probabilities for each token in the\n",
    "                     vocabulary.\n",
    "        x: A tuple containing the input data for the model:\n",
    "           - The first element is a dictionary with 'token_ids' and 'padding_mask'\n",
    "             for the source input tokens.\n",
    "           - The second element is a dictionary with 'token_ids' and 'padding_mask'\n",
    "             for the target input tokens.\n",
    "        y: A 3D array of shape (batch_size, sequence_length, n_vocab) representing\n",
    "           the one-hot encoded target token distributions.\n",
    "\n",
    "    Returns:\n",
    "        A scalar value representing the mean categorical cross-entropy loss over\n",
    "        the batch.\n",
    "    \n",
    "    Description:\n",
    "        This function computes the categorical cross-entropy loss between the predicted\n",
    "        token probabilities and the true target token distributions. It calculates the\n",
    "        loss only for tokens where the label is not equal to 5 (which is assumed to be\n",
    "        a padding token or ignored token). The loss is averaged over the non-masked\n",
    "        tokens and batch dimensions.\n",
    "\n",
    "    Example:\n",
    "        >>> loss = CategoricalCrossEntropy(model, (input_x, target_x), target_y)\n",
    "    \"\"\"\n",
    "    input_tokens = x[0]\n",
    "    output_tokens = x[1]\n",
    "    y_hat = transformer(input_tokens, output_tokens)\n",
    "    labels = jnp.argmax(y, axis=-1)\n",
    "    mask = labels != 5\n",
    "    return jnp.mean(\n",
    "        -(((y * jnp.log(y_hat)).sum(axis=-1)) * mask).sum(axis=-1, keepdims=True) /\n",
    "        mask.sum(axis=-1, keepdims=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4569f4d8-2746-44f5-9a8f-b95f739f0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x,max):\n",
    "        return jnp.array(x[:,:,None]==jnp.arange(max),dtype=jnp.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f2cdf99-5a24-4d40-b0e8-63460bbcbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import re\n",
    "import unicodedata\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    A class for tokenization and detokenization using SentencePiece.\n",
    "\n",
    "    This class supports loading a pre-trained SentencePiece model, cleaning text, \n",
    "    training a new model, and converting text to tokens and back. \n",
    "\n",
    "    Attributes:\n",
    "        model_prefix (str): Prefix used for the SentencePiece model file.\n",
    "        model_file (str): Full path to the SentencePiece model file.\n",
    "        sp (spm.SentencePieceProcessor): Instance of SentencePieceProcessor loaded \n",
    "            with the model file.\n",
    "\n",
    "    Args:\n",
    "        model_prefix (str): The prefix for the SentencePiece model file. The model file \n",
    "                            should be named `<model_prefix>.model`.\n",
    "\n",
    "    Methods:\n",
    "        clean_text(sentence: str) -> str:\n",
    "            Cleans and normalizes the input text by handling contractions, punctuation,\n",
    "            and whitespace.\n",
    "\n",
    "        batched_clean_text(x: list[str]) -> list[str]:\n",
    "            Applies `clean_text` to a list of sentences.\n",
    "\n",
    "        train(file_name: str, vocab_size: int):\n",
    "            Trains a SentencePiece model with the given file and vocabulary size, and\n",
    "            saves it with the specified model prefix.\n",
    "\n",
    "        __call__(x: str or list[str], out_type=None) -> list[int]:\n",
    "            Encodes the input text or list of texts into tokens using the trained SentencePiece model.\n",
    "\n",
    "        detokenize(tokens: list[int]) -> str:\n",
    "            Converts a list of token IDs back into a human-readable string.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the model file does not exist or cannot be loaded.\n",
    "\n",
    "    Example:\n",
    "        >>> tokenizer = Tokenizer(\"my_model_prefix\")\n",
    "        >>> tokenizer.train(\"data.txt\", vocab_size=5000)\n",
    "        >>> tokens = tokenizer(\"Hello, world!\")\n",
    "        >>> text = tokenizer.detokenize(tokens)\n",
    "    \"\"\"\n",
    "    import sentencepiece as spm\n",
    "    def __init__(self,model_prefix):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_file = self.model_prefix + \".model\"\n",
    "        try:\n",
    "            self.sp = spm.SentencePieceProcessor(model_file = self.model_file)\n",
    "        except:\n",
    "            print(\"Model File Not Found. Tokenizer must be trained in order to make changes.\")\n",
    "    @classmethod\n",
    "    def clean_text(cls,sentence):\n",
    "        pattern = r'[\\s]+'\n",
    "        sentence = re.sub(r\"\\s+\",\" \",re.sub(r\"([^\\'\\w])\",r\" \\1\",sentence))\n",
    "        contractions = {\"'ve\":\" have\",\n",
    "                    \"'ll\":\" will\",\n",
    "                    \"'m\":\" am\",\n",
    "                    \"'re\":\" are\",\n",
    "                    \"n't\":\" not\",\n",
    "                    \"'d\":\" had\"}\n",
    "        sentence = unicodedata.normalize(\"NFD\",sentence)\n",
    "        words = re.split(pattern,sentence)\n",
    "        for contraction in contractions:\n",
    "            words = [word.replace(contraction,contractions[contraction]) if contraction in word else word for word in words]\n",
    "        words = re.split(pattern,\" \".join(words))\n",
    "        return \" \".join(words)\n",
    "    @classmethod\n",
    "    def batched_clean_text(cls,x):\n",
    "        return [text for text in map(cls.clean_text,x)]\n",
    "        \n",
    "    def train(self,file_name,vocab_size):\n",
    "        spm.SentencePieceTrainer.train(input=file_name,\n",
    "                                       model_prefix =self.model_prefix,\n",
    "                                       vocab_size = vocab_size,\n",
    "                                       control_symbols='<start>,<end>,<pad>')\n",
    "        self.sp = spm.SentencePieceProcessor(model_file = self.model_file)\n",
    "    def __call__(self,x,out_type=None):\n",
    "        if type(x) == str:\n",
    "            x = Tokenizer.clean_text(x)   \n",
    "        else:\n",
    "            x = Tokenizer.batched_clean_text(x)\n",
    "        #print(x)\n",
    "        return self.sp.encode(x,out_type)\n",
    "    def detokenize(self,tokens):\n",
    "        return self.sp.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1052db9e-cc62-4366-8326-0f9c4d947be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padder:\n",
    "    \"\"\"\n",
    "    A class for padding and shifting token sequences for natural language processing tasks.\n",
    "\n",
    "    This class is designed to pad token sequences to a specified maximum length and to\n",
    "    perform left shifting operations required for various sequence processing tasks.\n",
    "\n",
    "    Attributes:\n",
    "        sp (spm.SentencePieceProcessor): Instance of SentencePieceProcessor used for token operations.\n",
    "        max_len (int): The maximum length to which sequences will be padded.\n",
    "        pad_token (int): ID of the padding token.\n",
    "        start_token (int): ID of the start token.\n",
    "        end_token (int): ID of the end token.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): An instance of the Tokenizer class to retrieve token IDs.\n",
    "        max_len (int): Maximum length of sequences after padding.\n",
    "\n",
    "    Methods:\n",
    "        add_pads(tokens: list[int], max_len: int = None) -> tuple[list[int], list[int]]:\n",
    "            Pads the input token sequence to a specified maximum length with special tokens.\n",
    "\n",
    "        left_shift(tokens: jnp.ndarray, pad_token: int) -> jnp.ndarray:\n",
    "            Shifts the token sequence to the left by one position, filling the last position\n",
    "            with the given padding token.\n",
    "\n",
    "        left_shift_mask(padding: jnp.ndarray) -> jnp.ndarray:\n",
    "            Shifts the padding mask to the left by one position, filling the last position\n",
    "            with zeros.\n",
    "\n",
    "        __call__(tokens: list[int] or list[list[int]]) -> dict:\n",
    "            Applies padding to a single sequence or a batch of sequences. Returns a dictionary\n",
    "            with token IDs and padding masks.\n",
    "\n",
    "    Example:\n",
    "        >>> tokenizer = Tokenizer(\"my_model_prefix\")\n",
    "        >>> padder = Padder(tokenizer, max_len=50)\n",
    "        >>> padded_tokens, pad_mask = padder.add_pads([1, 2, 3, 4])\n",
    "        >>> shifted_tokens = padder.left_shift(jnp.array([[1, 2, 3, 4]]), pad_token=0)\n",
    "        >>> padded_batch = padder([[1, 2, 3], [4, 5]])\n",
    "        >>> print(padded_batch[\"token_ids\"])  # List of padded sequences\n",
    "        >>> print(padded_batch[\"padding_mask\"])  # List of padding masks\n",
    "    \"\"\"\n",
    "    def __init__(self,tokenizer,max_len):\n",
    "        self.sp = tokenizer.sp\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = self.sp.piece_to_id(\"<pad>\")\n",
    "        self.start_token = self.sp.piece_to_id(\"<start>\")\n",
    "        self.end_token = self.sp.piece_to_id(\"<end>\")\n",
    "    def add_pads(self,tokens,max_len=None):\n",
    "        if max_len==None:\n",
    "            max_len = self.max_len\n",
    "        pad_mask = [1]*(len(tokens)+2)\n",
    "        if len(tokens)+2>=max_len:\n",
    "            return [self.start_token]+tokens[:max_len-2]+[self.end_token],pad_mask[:max_len]\n",
    "        pads_ = [self.pad_token]*(max_len-2-len(tokens))\n",
    "        pad_mask[max_len:] = [0]*len(pads_)\n",
    "        return [self.start_token]+tokens+[self.end_token]+pads_,pad_mask\n",
    "    @classmethod\n",
    "    def left_shift(cls,tokens,pad_token):\n",
    "        return jnp.concat([tokens[:,1:],jnp.expand_dims(jnp.repeat(jnp.array([pad_token]),tokens.shape[0]),axis=-1)],axis=-1)\n",
    "    @classmethod\n",
    "    def left_shift_mask(cls,padding):\n",
    "        return jnp.concat([padding[:,1:],jnp.expand_dims(jnp.repeat(jnp.array([0]),padding.shape[0]),axis=-1)],axis=-1)\n",
    "    def __call__(self,tokens):\n",
    "        if type(tokens[0])==int:\n",
    "            return self.add_pads(tokens)\n",
    "        else:\n",
    "            pad_map = list(map(self.add_pads,tokens))\n",
    "            return {\"token_ids\":[sentence for sentence,_ in pad_map],\n",
    "                    \"padding_mask\":[pad_mask for _,pad_mask in pad_map]}\n",
    "        \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1b7dfb34-0ab1-4c39-ab26-4613f055dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1b556827-6059-44ac-9606-6af183f7be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"language_translation_data/eng_-french.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9eea9445-78a9-43ae-bf95-0d2185d9410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = data[\"English words/sentences\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7b8afe08-d856-4d2d-b944-c7b5e38f39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_text = data[\"French words/sentences\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4e269515-8b5b-4537-8bb5-61a1f546b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = Tokenizer(\"en_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2b984248-c9c8-43f5-88f8-f98c0e350523",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(\"fr_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3f616993-dc1d-452c-b403-282638d78d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_en = Padder(en_tokenizer,max_len=64)\n",
    "padding_fr = Padder(fr_tokenizer,max_len=64)\n",
    "en_tokens = en_tokenizer(en_text)\n",
    "fr_tokens = fr_tokenizer(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1643d61e-14e9-4aa4-812b-a92f94a23c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text_en = padding_en(en_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "975347ce-8b48-416a-b8db-5909a6ee7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_text_fr = padding_fr(fr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "40be2c82-7cca-4144-af7a-b379f5316e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schedular:\n",
    "    \"\"\"\n",
    "    A class for learning rate scheduling in optimization.\n",
    "\n",
    "    This class adjusts the learning rate of an optimizer according to a specified\n",
    "    scheduling strategy. It uses a simple learning rate scheduler based on the\n",
    "    model dimensionality, the current training step, and a warmup step parameter.\n",
    "\n",
    "    Attributes:\n",
    "        optimizer (Optimizer): The optimizer instance whose learning rate will be adjusted.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): An instance of an optimizer whose learning rate needs to be adjusted.\n",
    "\n",
    "    Methods:\n",
    "        update(d_model: int, step: int, warmup_steps: int) -> float:\n",
    "            Updates the learning rate of the optimizer based on the current step and warmup steps.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = AdamW(lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, lambda_=0.01)\n",
    "        >>> schedular = Schedular(optimizer)\n",
    "        >>> lr = schedular.update(d_model=512, step=1000, warmup_steps=4000)\n",
    "        >>> print(f\"Updated Learning Rate: {lr}\")\n",
    "    \"\"\"\n",
    "    def __init__(self,optimizer):\n",
    "        self.optimizer = optimizer\n",
    "    def update(self,d_model,step,warmup_steps):\n",
    "        self.optimizer.lr = (d_model**-0.5)*min(step**-0.5,step*warmup_steps**-1.5)\n",
    "        return self.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b9456-e914-4b2f-8373-04f4299c1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 \n",
      "Loss:\n",
      "6.8926473\n",
      "Epoch  1 \n",
      "Loss:\n",
      "6.8774495\n",
      "Epoch  2 \n",
      "Loss:\n",
      "6.8621492\n",
      "Epoch  3 \n",
      "Loss:\n",
      "6.8467507\n",
      "Epoch  4 \n",
      "Loss:\n",
      "6.8311764\n",
      "Epoch  5 \n",
      "Loss:\n",
      "6.8154383\n",
      "Epoch  6 \n",
      "Loss:\n",
      "6.7997185\n",
      "Epoch  7 \n",
      "Loss:\n",
      "6.7837515\n",
      "Epoch  8 \n",
      "Loss:\n",
      "6.7678725\n",
      "Epoch  9 \n",
      "Loss:\n",
      "6.7518153\n",
      "Epoch  10 \n",
      "Loss:\n",
      "6.7358557\n",
      "Epoch  11 \n",
      "Loss:\n",
      "6.7200174\n",
      "Epoch  12 \n",
      "Loss:\n",
      "6.7041483\n",
      "Epoch  13 \n",
      "Loss:\n",
      "6.6878127\n",
      "Epoch  14 \n",
      "Loss:\n",
      "6.6718594\n",
      "Epoch  15 \n",
      "Loss:\n",
      "6.6560326\n",
      "Epoch  16 \n",
      "Loss:\n",
      "6.6401478\n",
      "Epoch  17 \n",
      "Loss:\n",
      "6.6241474\n",
      "Epoch  18 \n",
      "Loss:\n",
      "6.6085376\n",
      "Epoch  19 \n",
      "Loss:\n",
      "6.5932787\n",
      "Epoch  20 \n",
      "Loss:\n",
      "6.5780363\n",
      "Epoch  21 \n",
      "Loss:\n",
      "6.5614834\n",
      "Epoch  22 \n",
      "Loss:\n",
      "6.5458536\n",
      "Epoch  23 \n",
      "Loss:\n",
      "6.5306344\n",
      "Epoch  24 \n",
      "Loss:\n",
      "6.5144577\n",
      "Epoch  25 \n",
      "Loss:\n",
      "6.5001183\n",
      "Epoch  26 \n",
      "Loss:\n",
      "6.4854458\n",
      "Epoch  27 \n",
      "Loss:\n",
      "6.4694576\n",
      "Epoch  28 \n",
      "Loss:\n",
      "6.4542513\n",
      "Epoch  29 \n",
      "Loss:\n",
      "6.4409345\n",
      "Epoch  30 \n",
      "Loss:\n",
      "6.4236565\n",
      "Epoch  31 \n",
      "Loss:\n",
      "6.4095564\n",
      "Epoch  32 \n",
      "Loss:\n",
      "6.3970894\n",
      "Epoch  33 \n",
      "Loss:\n",
      "6.3811674\n",
      "Epoch  34 \n",
      "Loss:\n",
      "6.3659296\n",
      "Epoch  35 \n",
      "Loss:\n",
      "6.3526234\n",
      "Epoch  36 \n",
      "Loss:\n",
      "6.3392717\n",
      "Epoch  37 \n",
      "Loss:\n",
      "6.3229265\n",
      "Epoch  38 \n",
      "Loss:\n",
      "6.3088613\n",
      "Epoch  39 \n",
      "Loss:\n",
      "6.2956605\n",
      "Epoch  40 \n",
      "Loss:\n",
      "6.2817375\n",
      "Epoch  41 \n",
      "Loss:\n",
      "6.2682385\n",
      "Epoch  42 \n",
      "Loss:\n",
      "6.2576057\n",
      "Epoch  43 \n",
      "Loss:\n",
      "6.2399936\n",
      "Epoch  44 \n",
      "Loss:\n",
      "6.2278867\n",
      "Epoch  45 \n",
      "Loss:\n",
      "6.2133093\n",
      "Epoch  46 \n",
      "Loss:\n",
      "6.2002953\n",
      "Epoch  47 \n",
      "Loss:\n",
      "6.1852494\n",
      "Epoch  48 \n",
      "Loss:\n",
      "6.1745243\n",
      "Epoch  49 \n",
      "Loss:\n",
      "6.1622114\n",
      "Epoch  50 \n",
      "Loss:\n",
      "6.1493187\n",
      "Epoch  51 \n",
      "Loss:\n",
      "6.1351457\n",
      "Epoch  52 \n",
      "Loss:\n",
      "6.1195677\n",
      "Epoch  53 \n",
      "Loss:\n",
      "6.1072273\n",
      "Epoch  54 \n",
      "Loss:\n",
      "6.0973725\n",
      "Epoch  55 \n",
      "Loss:\n",
      "6.0826087\n",
      "Epoch  56 \n",
      "Loss:\n",
      "6.0719805\n",
      "Epoch  57 \n",
      "Loss:\n",
      "6.0597906\n",
      "Epoch  58 \n",
      "Loss:\n",
      "6.0462734\n",
      "Epoch  59 \n",
      "Loss:\n",
      "6.0343614\n",
      "Epoch  60 \n",
      "Loss:\n",
      "6.0216856\n",
      "Epoch  61 \n",
      "Loss:\n",
      "6.0093446\n",
      "Epoch  62 \n",
      "Loss:\n",
      "5.8882685\r"
     ]
    }
   ],
   "source": [
    "Trans = Transformer(512,2048,8,1,1000)\n",
    "en_tokens,en_mask = padded_text_en['token_ids'][:1024],padded_text_en['padding_mask'][:1024]\n",
    "fr_tokens,fr_mask = padded_text_fr['token_ids'][:1024],padded_text_fr['padding_mask'][:1024]\n",
    "opt = AdamW(1e-2,9e-1,999e-3,1e-9,1e-2)\n",
    "trainer = Trainer(Trans,CategoricalCrossEntropy,opt)\n",
    "trainer.train(({'token_ids':en_tokens,'padding_mask':en_mask},\n",
    "                {'token_ids':fr_tokens,'padding_mask':fr_mask}),\n",
    "               fr_tokens,500,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02499a3e-ec66-454c-b5e5-ed17934338c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    \"\"\"\n",
    "    A class for translating text from one language to another using a neural translation model.\n",
    "\n",
    "    This class takes in a translation model and tokenizers for both source and target languages,\n",
    "    along with padding utilities. It performs the translation by tokenizing the input text, \n",
    "    padding the tokens, generating predictions from the model, and detokenizing the predicted tokens.\n",
    "\n",
    "    Attributes:\n",
    "        model (Transformer): The translation model used for generating predictions.\n",
    "        en_tokenizer (Tokenizer): Tokenizer for the source language (e.g., English).\n",
    "        fr_tokenizer (Tokenizer): Tokenizer for the target language (e.g., French).\n",
    "        en_padder (Padder): Padding utility for the source language tokens.\n",
    "        fr_padder (Padder): Padding utility for the target language tokens.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): A model instance used for translation.\n",
    "        en_tokenizer (Tokenizer): Tokenizer instance for the source language.\n",
    "        fr_tokenizer (Tokenizer): Tokenizer instance for the target language.\n",
    "        en_padder (Padder): Padding utility for the source language.\n",
    "        fr_padder (Padder): Padding utility for the target language.\n",
    "\n",
    "    Methods:\n",
    "        __call__(text: str) -> None:\n",
    "            Translates the given text from the source language to the target language and prints the result.\n",
    "\n",
    "    Example:\n",
    "        >>> en_tokenizer = Tokenizer(\"en_model\")\n",
    "        >>> fr_tokenizer = Tokenizer(\"fr_model\")\n",
    "        >>> en_padder = Padder(en_tokenizer, max_len=128)\n",
    "        >>> fr_padder = Padder(fr_tokenizer, max_len=128)\n",
    "        >>> model = Transformer(d_model=512, d_ff=2048, num_heads=8, num_layers=6, n_vocab=30000)\n",
    "        >>> translator = Translator(model, en_tokenizer, fr_tokenizer, en_padder, fr_padder)\n",
    "        >>> translator(\"Hello, how are you?\")\n",
    "        Translates \"Hello, how are you?\" to the target language and prints the result.\n",
    "    \"\"\"\n",
    "    def __init__(self,model,en_tokenizer,fr_tokenizer,en_padder,fr_padder):\n",
    "        self.model = model\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_padder = en_padder\n",
    "        self.fr_padder = fr_padder\n",
    "    def __call__(self,text):\n",
    "        en_tokens = self.en_tokenizer([text])\n",
    "        #print(en_tokens)\n",
    "        padded_text_en = self.en_padder(en_tokens)\n",
    "        print(padded_text_en)\n",
    "        #fr_tokens = [[3]]\n",
    "        count = 0\n",
    "        pred_token = 0\n",
    "        fr_text = \"A\"\n",
    "        while pred_token!=4:\n",
    "            fr_tokens = self.fr_tokenizer([fr_text])\n",
    "            padded_text_fr = self.fr_padder(fr_tokens)\n",
    "            #print(padded_text_fr['token_ids'])\n",
    "            index=padded_text_fr['token_ids'][0].index(4)\n",
    "            padded_text_fr['token_ids'][0][index] = 5\n",
    "            padded_text_fr['padding_mask'][0][index] = 0\n",
    "            print(padded_text_fr)\n",
    "            predicted_tokens = jnp.argmax(self.model({'token_ids':padded_text_en['token_ids'],'padding_mask':padded_text_en['padding_mask']},\n",
    "                   {'token_ids':padded_text_fr['token_ids'],'padding_mask':padded_text_fr['padding_mask']}),axis=-1)\n",
    "            #print(predicted_tokens)\n",
    "            pred_token = predicted_tokens[0][count]\n",
    "            fr_text+=self.fr_tokenizer.detokenize(pred_token.tolist())\n",
    "            count+=1\n",
    "        #print(predicted_tokens)\n",
    "        print([self.fr_tokenizer.detokenize(tokens.tolist()) for tokens in predicted_tokens])\n",
    "        print(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef8fc3a7-80b1-4beb-9245-512db27f19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxlib\n",
    "def convert_weights(params):\n",
    "    params = copy.deepcopy(params)\n",
    "    for key in params:\n",
    "        if type(params[key])==jaxlib.xla_extension.ArrayImpl:\n",
    "            params[key]=params[key].tolist()\n",
    "        else:\n",
    "            params[key] = convert_weights(params[key])\n",
    "    return params\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4789e779-d252-4934-92de-a84618e5d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = convert_weights(Trans.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90b37759-7910-46b5-9ced-6d00fa30517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_string = json.dumps(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b6186df-981d-48ad-bdc7-b83c9205fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights-6-512.json\",\"w\") as file:\n",
    "    file.write(weight_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "d4b0efa5-82aa-4f66-848e-fa96012a19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weights_jax(params):\n",
    "    params = copy.deepcopy(params)\n",
    "    for key in params:\n",
    "        if type(params[key])==list:\n",
    "            params[key]=jnp.array(params[key])\n",
    "        else:\n",
    "            params[key] = convert_weights_jax(params[key])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "9056f1af-d627-4f6e-94c1-b0976a4949a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights.json\",\"r\") as file:\n",
    "    weight_string = file.read()\n",
    "    params = json.loads(weight_string)\n",
    "    params = convert_weights_jax(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "f988db7e-5b41-48eb-a95b-a9b9ffcebfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7dba4c54-a2fc-4e37-9d96-7170cce744bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clip_gradients(params):\n",
    "#     params = copy.deepcopy(params)\n",
    "#     for key in params:\n",
    "#         if type(params[key])==jaxlib.xla_extension.ArrayImpl:\n",
    "#             params[key] = jnp.clip(params[key],-1.0,1.0)\n",
    "#             params[key] = jnp.nan_to_num(params[key])\n",
    "#         else:\n",
    "#             params[key] = clip_gradients(params[key])\n",
    "#     return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4218358-f49a-4798-ba61-3af20d21cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa7094-b73f-4cf4-86cf-71d3018cfe4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
